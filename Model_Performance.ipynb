{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, cohen_kappa_score, balanced_accuracy_score\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, GRU, SimpleRNN, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import time\n",
        "import psutil\n",
        "import warnings\n",
        "import shap\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define data paths\n",
        "macro_folder = '/content/drive/MyDrive/processed_data'\n",
        "financial_folder = '/content/drive/MyDrive/financial_info'\n",
        "\n",
        "# 1. DATA LOADING\n",
        "def load_country_data(country_name):\n",
        "    \"\"\"Load stock and macro data for a specific country\"\"\"\n",
        "    country_lower = country_name.lower()\n",
        "    stock_file = f'{financial_folder}/{country_lower}_stock.csv'\n",
        "    macro_file = f'{macro_folder}/{country_lower}_macro.csv'\n",
        "\n",
        "    try:\n",
        "        stock_df = pd.read_csv(stock_file, parse_dates=['Date'], skiprows=[1])\n",
        "        stock_df.columns = [col.lower() for col in stock_df.columns]\n",
        "        if 'date' not in stock_df.columns and 'Date' in stock_df.columns:\n",
        "            stock_df = stock_df.rename(columns={'Date': 'date'})\n",
        "        stock_df['close'] = pd.to_numeric(stock_df['close'], errors='coerce')\n",
        "\n",
        "        macro_df = pd.read_csv(macro_file, parse_dates=['date'])\n",
        "        macro_df.columns = [col.lower() for col in macro_df.columns]\n",
        "\n",
        "        print(f\"Loaded {country_name} data: {stock_df.shape[0]} stock rows, {macro_df.shape[0]} macro rows\")\n",
        "        return stock_df, macro_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data for {country_name}: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "# 2. PREPROCESSING\n",
        "def preprocess_country_data(stock_df, macro_df, country_name, include_macro=True):\n",
        "    \"\"\"Preprocess stock and macro data for model training\"\"\"\n",
        "    stock_df = stock_df.copy()\n",
        "    macro_df = macro_df.copy()\n",
        "\n",
        "    # Calculate returns and volatility features\n",
        "    stock_df['return_1q'] = stock_df['close'].pct_change(63)\n",
        "    stock_df['return_2q'] = stock_df['close'].pct_change(126)\n",
        "    stock_df['volatility_2q'] = stock_df['return_1q'].rolling(126).std()\n",
        "\n",
        "    # Add monthly metrics as fallback\n",
        "    stock_df['return_1m'] = stock_df['close'].pct_change(21)\n",
        "    stock_df['return_3m'] = stock_df['close'].pct_change(63)\n",
        "    stock_df['volatility'] = stock_df['return_1m'].rolling(21).std()\n",
        "\n",
        "    # Technical indicators\n",
        "    stock_df['ma_ratio'] = stock_df['close'] / stock_df['close'].rolling(20).mean()\n",
        "    stock_df['momentum'] = stock_df['return_1m'].rolling(3).sum()\n",
        "\n",
        "    processed_df = stock_df.copy()\n",
        "\n",
        "    if include_macro:\n",
        "        # Process macro data\n",
        "        macro_cols_to_keep = [col for col in macro_df.columns\n",
        "                             if not col.endswith('_x') and not col.endswith('_y') and col != 'date']\n",
        "        print(f\"Available macro columns for {country_name}: {macro_cols_to_keep}\")\n",
        "\n",
        "        macro_df = macro_df[['date'] + macro_cols_to_keep]\n",
        "        for col in macro_cols_to_keep:\n",
        "            macro_df[col] = pd.to_numeric(macro_df[col], errors='coerce')\n",
        "\n",
        "        # Forward fill missing values in macro data\n",
        "        macro_df = macro_df.sort_values('date').set_index('date')\n",
        "        macro_df = macro_df.ffill().bfill()\n",
        "        macro_df = macro_df.reset_index()\n",
        "\n",
        "        # Merge stock and macro data\n",
        "        processed_df = pd.merge_asof(stock_df.sort_values('date'),\n",
        "                                     macro_df.sort_values('date'), on='date')\n",
        "\n",
        "        # Calculate percentage changes for macro columns\n",
        "        for col in macro_cols_to_keep:\n",
        "            if col in processed_df.columns:\n",
        "                processed_df[f'{col}_change'] = processed_df[col].pct_change(1).fillna(0)\n",
        "\n",
        "    # Fill missing values\n",
        "    for col in processed_df.columns:\n",
        "        if col not in ['date', 'country']:\n",
        "            if col.endswith('_change'):\n",
        "                processed_df[col] = processed_df[col].fillna(0)\n",
        "            else:\n",
        "                processed_df[col] = processed_df[col].ffill().bfill()\n",
        "\n",
        "    processed_df['country'] = country_name\n",
        "    print(f\"Successfully processed {country_name} data: {processed_df.shape[0]} rows, {processed_df.shape[1]} columns\")\n",
        "    return processed_df\n",
        "\n",
        "# 3. REGIME CLASSIFICATION\n",
        "def define_market_regime(df, method='adaptive'):\n",
        "    \"\"\"Define market regimes using adaptive thresholds\"\"\"\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Check if quarterly metrics are available\n",
        "    has_quarterly = (df_copy['return_1q'].notna().sum() > len(df_copy) * 0.5 and\n",
        "                    df_copy['volatility_2q'].notna().sum() > len(df_copy) * 0.5)\n",
        "\n",
        "    if has_quarterly:\n",
        "        print(\"Using quarterly metrics for regime definition\")\n",
        "        return_col = 'return_1q'\n",
        "        vol_col = 'volatility_2q'\n",
        "    else:\n",
        "        print(\"Quarterly metrics not available, using monthly metrics\")\n",
        "        return_col = 'return_1m'\n",
        "        vol_col = 'volatility'\n",
        "\n",
        "    # Remove rows with missing values in key columns\n",
        "    mask = df_copy[return_col].notna() & df_copy[vol_col].notna()\n",
        "    valid_data = df_copy[mask]\n",
        "\n",
        "    if method == 'adaptive':\n",
        "        # Adaptive thresholds based on historical distribution\n",
        "        return_high = max(valid_data[return_col].quantile(0.7), 0.03)\n",
        "        return_low = min(valid_data[return_col].quantile(0.3), -0.02)\n",
        "        vol_high = valid_data[vol_col].quantile(0.65)\n",
        "\n",
        "        # Define conditions for regimes (0: Bear, 1: Neutral, 2: Bull)\n",
        "        conditions = [\n",
        "            (df_copy[return_col] < return_low) & (df_copy[vol_col] > vol_high),\n",
        "            (df_copy[return_col] >= return_low) & (df_copy[return_col] <= return_high),\n",
        "            (df_copy[return_col] > return_high)\n",
        "        ]\n",
        "    else:\n",
        "        # Quantile-based thresholds\n",
        "        return_high = valid_data[return_col].quantile(0.65)\n",
        "        return_low = valid_data[return_col].quantile(0.35)\n",
        "        vol_high = valid_data[vol_col].quantile(0.65)\n",
        "\n",
        "        conditions = [\n",
        "            (df_copy[return_col] < return_low) & (df_copy[vol_col] > vol_high),\n",
        "            (df_copy[return_col] >= return_low) & (df_copy[return_col] <= return_high),\n",
        "            (df_copy[return_col] > return_high)\n",
        "        ]\n",
        "\n",
        "    choices = [0, 1, 2]  # 0: Bear, 1: Neutral, 2: Bull\n",
        "    df_copy['regime'] = np.select(conditions, choices, default=1)\n",
        "\n",
        "    # Print regime distribution\n",
        "    regime_counts = df_copy['regime'].value_counts()\n",
        "    print(f\"Regime distribution: {dict(regime_counts)}\")\n",
        "\n",
        "    # Ensure balanced class distribution\n",
        "    if len(regime_counts) < 3:\n",
        "        print(\"WARNING: Not all regimes are present in the data!\")\n",
        "        missing_regimes = set([0, 1, 2]) - set(regime_counts.index)\n",
        "        print(f\"Missing regimes: {missing_regimes}\")\n",
        "\n",
        "        if 0 not in regime_counts.index:\n",
        "            worst_returns = df_copy[df_copy[return_col].notna()].nsmallest(max(5, int(len(df_copy) * 0.1)), return_col).index\n",
        "            df_copy.loc[worst_returns, 'regime'] = 0\n",
        "            print(f\"Forced {len(worst_returns)} data points to be bear regime for balanced classification\")\n",
        "\n",
        "    regime_counts = df_copy['regime'].value_counts()\n",
        "    print(f\"Final regime distribution: {dict(regime_counts)}\")\n",
        "    return df_copy\n",
        "\n",
        "# 4. FEATURE PREPARATION\n",
        "def prepare_features(df, include_macro=True):\n",
        "    \"\"\"Prepare features for model training\"\"\"\n",
        "    # Select appropriate features based on data availability\n",
        "    if 'return_1q' in df.columns and df['return_1q'].notna().sum() > len(df) * 0.5:\n",
        "        base_features = ['return_1q', 'return_2q', 'volatility_2q', 'ma_ratio', 'momentum']\n",
        "    else:\n",
        "        base_features = ['return_1m', 'return_3m', 'volatility', 'ma_ratio', 'momentum']\n",
        "\n",
        "    # Add macro features if requested\n",
        "    macro_features = []\n",
        "    if include_macro:\n",
        "        possible_macro = ['gdp', 'cpi', 'exchange_rate', 'imports', 'exports',\n",
        "                         'gdp_change', 'cpi_change', 'imports_change', 'exports_change',\n",
        "                         'interest_rate', 'interest_rate_change']\n",
        "        macro_features = [f for f in possible_macro if f in df.columns and df[f].notna().sum() > len(df) * 0.3]\n",
        "\n",
        "    # Combine all features\n",
        "    feature_names = base_features + macro_features\n",
        "    available_features = [f for f in feature_names if f in df.columns]\n",
        "\n",
        "    # Remove empty or zero-variance features\n",
        "    non_null_counts = df[available_features].count()\n",
        "    empty_columns = non_null_counts[non_null_counts == 0].index.tolist()\n",
        "    if empty_columns:\n",
        "        print(f\"Removing completely empty features: {empty_columns}\")\n",
        "        available_features = [f for f in available_features if f not in empty_columns]\n",
        "\n",
        "    # Handle missing values\n",
        "    df_clean = df.copy().sort_values('date')\n",
        "    for col in available_features:\n",
        "        df_clean[col] = df_clean[col].ffill()\n",
        "        if df_clean[col].isna().any():\n",
        "            if col.endswith('_change'):\n",
        "                df_clean[col] = df_clean[col].fillna(0)\n",
        "            else:\n",
        "                df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
        "\n",
        "    X = df_clean[available_features].copy()\n",
        "    y = df_clean['regime'].copy()\n",
        "    return X, y, available_features\n",
        "\n",
        "# 5. MODEL EVALUATION METRICS\n",
        "def calculate_model_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculate comprehensive model evaluation metrics\"\"\"\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
        "        'f1_score': f1_score(y_true, y_pred, average='weighted'),\n",
        "        'precision': precision_score(y_true, y_pred, average='weighted'),\n",
        "        'recall': recall_score(y_true, y_pred, average='weighted'),\n",
        "        'kappa': cohen_kappa_score(y_true, y_pred),\n",
        "        'confusion_matrix': confusion_matrix(y_true, y_pred)\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "# 6. TRADITIONAL MODEL TRAINING\n",
        "def train_traditional_models(X_train, X_test, y_train, y_test, feature_names):\n",
        "    \"\"\"Train and evaluate traditional ML models\"\"\"\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Initialize models\n",
        "    models = {\n",
        "        'RandomForest': RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42),\n",
        "        'SVC': SVC(probability=True, class_weight='balanced', random_state=42),\n",
        "        'XGBoost': XGBClassifier(n_estimators=200, learning_rate=0.1, random_state=42,\n",
        "                                scale_pos_weight=1, use_label_encoder=False, eval_metric='mlogloss')\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    start_time = time.time()\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "        model_start_time = time.time()\n",
        "        mem_before = psutil.virtual_memory().percent\n",
        "\n",
        "        try:\n",
        "            # Train model\n",
        "            model.fit(X_train_scaled, y_train)\n",
        "\n",
        "            # Make predictions\n",
        "            y_train_pred = model.predict(X_train_scaled)\n",
        "            y_test_pred = model.predict(X_test_scaled)\n",
        "\n",
        "            # Calculate metrics\n",
        "            train_metrics = calculate_model_metrics(y_train, y_train_pred)\n",
        "            test_metrics = calculate_model_metrics(y_test, y_test_pred)\n",
        "\n",
        "            # Calculate training time and memory usage\n",
        "            training_time = time.time() - model_start_time\n",
        "            mem_after = psutil.virtual_memory().percent\n",
        "            mem_usage = mem_after - mem_before\n",
        "\n",
        "            # Store results\n",
        "            results[name] = {\n",
        "                'model': model,\n",
        "                'train_metrics': train_metrics,\n",
        "                'test_metrics': test_metrics,\n",
        "                'y_pred': y_test_pred,\n",
        "                'y_true': y_test,\n",
        "                'training_time': training_time,\n",
        "                'memory_usage': mem_usage\n",
        "            }\n",
        "\n",
        "            # Get feature importance for tree-based models\n",
        "            if name in ['RandomForest', 'XGBoost'] and hasattr(model, 'feature_importances_'):\n",
        "                results[name]['feature_importance'] = dict(zip(feature_names, model.feature_importances_))\n",
        "\n",
        "            print(f\"{name} - Train Accuracy: {train_metrics['accuracy']:.4f}, Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "            print(f\"{name} - Train Balanced Acc: {train_metrics['balanced_accuracy']:.4f}, Test Balanced Acc: {test_metrics['balanced_accuracy']:.4f}\")\n",
        "            print(f\"{name} - Training Time: {training_time:.2f}s, Memory Usage: {mem_usage:.2f}%\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training {name}: {str(e)}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# 7. DEEP LEARNING MODEL TRAINING\n",
        "def create_sequences(data, target, seq_length=10):\n",
        "    \"\"\"Create sequences for RNN models\"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(target[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def train_deep_learning_models(X_train, X_test, y_train, y_test, feature_names):\n",
        "    \"\"\"Train and evaluate deep learning models\"\"\"\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Create sequences for RNN models\n",
        "    seq_length = min(10, X_train_scaled.shape[0] // 5)\n",
        "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train.values, seq_length)\n",
        "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test.values, seq_length)\n",
        "\n",
        "    # Check if we have enough data for sequences\n",
        "    if len(X_train_seq) < 10 or len(X_test_seq) < 5:\n",
        "        print(\"WARNING: Not enough data for sequence models. Skipping deep learning models.\")\n",
        "        return {}\n",
        "\n",
        "    # One-hot encode target\n",
        "    num_classes = len(np.unique(np.concatenate([y_train, y_test])))\n",
        "    y_train_onehot = tf.keras.utils.to_categorical(y_train_seq, num_classes=num_classes)\n",
        "    y_test_onehot = tf.keras.utils.to_categorical(y_test_seq, num_classes=num_classes)\n",
        "\n",
        "    # Define models\n",
        "    input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "    output_shape = y_train_onehot.shape[1]\n",
        "\n",
        "    models = {\n",
        "        'LSTM': Sequential([\n",
        "            LSTM(64, input_shape=input_shape, return_sequences=True),\n",
        "            Dropout(0.3),\n",
        "            LSTM(32),\n",
        "            Dropout(0.3),\n",
        "            Dense(output_shape, activation='softmax')\n",
        "        ]),\n",
        "        'GRU': Sequential([\n",
        "            GRU(64, input_shape=input_shape, return_sequences=True),\n",
        "            Dropout(0.3),\n",
        "            GRU(32),\n",
        "            Dropout(0.3),\n",
        "            Dense(output_shape, activation='softmax')\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "        model_start_time = time.time()\n",
        "        mem_before = psutil.virtual_memory().percent\n",
        "\n",
        "        try:\n",
        "            # Compile model\n",
        "            model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                         loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "            # Early stopping\n",
        "            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "            # Train model\n",
        "            batch_size = min(32, X_train_seq.shape[0] // 4)\n",
        "            if batch_size < 1:\n",
        "                batch_size = 1\n",
        "\n",
        "            history = model.fit(X_train_seq, y_train_onehot,\n",
        "                              epochs=50,\n",
        "                              batch_size=batch_size,\n",
        "                              validation_split=0.2,\n",
        "                              callbacks=[early_stopping],\n",
        "                              verbose=1)\n",
        "\n",
        "            # Make predictions\n",
        "            y_train_pred_prob = model.predict(X_train_seq)\n",
        "            y_test_pred_prob = model.predict(X_test_seq)\n",
        "\n",
        "            y_train_pred = np.argmax(y_train_pred_prob, axis=1)\n",
        "            y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
        "\n",
        "            y_train_true = np.argmax(y_train_onehot, axis=1)\n",
        "            y_test_true = np.argmax(y_test_onehot, axis=1)\n",
        "\n",
        "            # Calculate metrics\n",
        "            train_metrics = calculate_model_metrics(y_train_true, y_train_pred)\n",
        "            test_metrics = calculate_model_metrics(y_test_true, y_test_pred)\n",
        "\n",
        "            # Calculate training time and memory usage\n",
        "            training_time = time.time() - model_start_time\n",
        "            mem_after = psutil.virtual_memory().percent\n",
        "            mem_usage = mem_after - mem_before\n",
        "\n",
        "            # Store results\n",
        "            results[name] = {\n",
        "                'model': model,\n",
        "                'train_metrics': train_metrics,\n",
        "                'test_metrics': test_metrics,\n",
        "                'history': history.history,\n",
        "                'y_pred': y_test_pred,\n",
        "                'y_true': y_test_true,\n",
        "                'training_time': training_time,\n",
        "                'memory_usage': mem_usage\n",
        "            }\n",
        "\n",
        "            print(f\"{name} - Train Accuracy: {train_metrics['accuracy']:.4f}, Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "            print(f\"{name} - Train Balanced Acc: {train_metrics['balanced_accuracy']:.4f}, Test Balanced Acc: {test_metrics['balanced_accuracy']:.4f}\")\n",
        "            print(f\"{name} - Training Time: {training_time:.2f}s, Memory Usage: {mem_usage:.2f}%\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training {name}: {str(e)}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# 8. PORTFOLIO PERFORMANCE SIMULATION\n",
        "def simulate_portfolio(df, predictions, start_date, end_date):\n",
        "    \"\"\"Simulate portfolio performance based on model predictions\"\"\"\n",
        "    # Filter data for simulation period\n",
        "    sim_df = df[(df['date'] >= start_date) & (df['date'] <= end_date)].copy()\n",
        "    sim_df = sim_df.sort_values('date')\n",
        "\n",
        "    # Calculate daily returns\n",
        "    sim_df['daily_return'] = sim_df['close'].pct_change()\n",
        "\n",
        "    # Initialize portfolio returns\n",
        "    portfolio_returns = {\n",
        "        'Buy and Hold': [],\n",
        "        **{model_name: [] for model_name in predictions.keys()}\n",
        "    }\n",
        "\n",
        "    # Calculate cumulative returns\n",
        "    cum_return_bh = 1.0\n",
        "    cum_returns_models = {model_name: 1.0 for model_name in predictions.keys()}\n",
        "\n",
        "    # Track positions for transaction costs\n",
        "    current_positions = {model_name: None for model_name in predictions.keys()}\n",
        "    transaction_cost = 0.001  # 0.1% per transaction\n",
        "\n",
        "    # Simulate portfolio performance\n",
        "    for i in range(1, len(sim_df)):\n",
        "        date = sim_df.iloc[i]['date']\n",
        "        daily_return = sim_df.iloc[i]['daily_return']\n",
        "\n",
        "        # Buy and Hold strategy\n",
        "        if not np.isnan(daily_return):\n",
        "            cum_return_bh *= (1 + daily_return)\n",
        "            portfolio_returns['Buy and Hold'].append({\n",
        "                'date': date,\n",
        "                'cumulative_return': cum_return_bh\n",
        "            })\n",
        "\n",
        "        # Model-based strategies\n",
        "        for model_name, model_preds in predictions.items():\n",
        "            if i-1 < len(model_preds):\n",
        "                pred_regime = model_preds[i-1]\n",
        "\n",
        "                # Investment strategy based on regime\n",
        "                # 0: Bear - Short the market (-1x exposure)\n",
        "                # 1: Neutral - Cash (0x exposure)\n",
        "                # 2: Bull - Long the market (1x exposure)\n",
        "                new_exposure = -1.0 if pred_regime == 0 else (0.0 if pred_regime == 1 else 1.0)\n",
        "\n",
        "                # Apply transaction costs if position changed\n",
        "                if current_positions[model_name] is not None and current_positions[model_name] != new_exposure:\n",
        "                    cum_returns_models[model_name] *= (1 - transaction_cost)\n",
        "\n",
        "                current_positions[model_name] = new_exposure\n",
        "\n",
        "                if not np.isnan(daily_return):\n",
        "                    model_return = daily_return * new_exposure\n",
        "                    cum_returns_models[model_name] *= (1 + model_return)\n",
        "                    portfolio_returns[model_name].append({\n",
        "                        'date': date,\n",
        "                        'cumulative_return': cum_returns_models[model_name]\n",
        "                    })\n",
        "\n",
        "    # Convert to DataFrames\n",
        "    for strategy, returns in portfolio_returns.items():\n",
        "        if returns:\n",
        "            portfolio_returns[strategy] = pd.DataFrame(returns)\n",
        "        else:\n",
        "            portfolio_returns[strategy] = pd.DataFrame(columns=['date', 'cumulative_return'])\n",
        "\n",
        "    return portfolio_returns\n",
        "\n",
        "# 9. PERFORMANCE EVALUATION\n",
        "def evaluate_portfolio_performance(portfolio_returns):\n",
        "    \"\"\"Calculate performance metrics for portfolios\"\"\"\n",
        "    performance = {}\n",
        "\n",
        "    for strategy, returns_df in portfolio_returns.items():\n",
        "        if returns_df.empty:\n",
        "            continue\n",
        "\n",
        "        # Calculate daily returns\n",
        "        returns_df['daily_return'] = returns_df['cumulative_return'].pct_change()\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        total_return = returns_df['cumulative_return'].iloc[-1] / returns_df['cumulative_return'].iloc[0] - 1\n",
        "        annual_return = (1 + total_return) ** (252 / len(returns_df)) - 1\n",
        "        volatility = returns_df['daily_return'].std() * np.sqrt(252)\n",
        "        sharpe_ratio = annual_return / volatility if volatility > 0 else 0\n",
        "\n",
        "        # Calculate maximum drawdown\n",
        "        cumulative = returns_df['cumulative_return']\n",
        "        running_max = cumulative.cummax()\n",
        "        drawdown = (cumulative / running_max - 1)\n",
        "        max_drawdown = drawdown.min()\n",
        "\n",
        "        # Calculate win rate\n",
        "        win_rate = (returns_df['daily_return'] > 0).mean()\n",
        "\n",
        "        performance[strategy] = {\n",
        "            'Total Return': total_return,\n",
        "            'Annual Return': annual_return,\n",
        "            'Volatility': volatility,\n",
        "            'Sharpe Ratio': sharpe_ratio,\n",
        "            'Max Drawdown': max_drawdown,\n",
        "            'Win Rate': win_rate\n",
        "        }\n",
        "\n",
        "    return pd.DataFrame(performance).T\n",
        "\n",
        "# 10. CROSS-COUNTRY COMPARISON\n",
        "def analyze_macro_value_added(simulation_results):\n",
        "    \"\"\"Analyze the value added by macroeconomic factors across countries\"\"\"\n",
        "    macro_value_analysis = {}\n",
        "\n",
        "    for country, results in simulation_results.items():\n",
        "        perf_metrics = results['performance_metrics']\n",
        "\n",
        "        for model_type in ['RandomForest', 'SVC', 'XGBoost', 'LSTM', 'GRU']:\n",
        "            with_macro = f\"{model_type}_with_macro\"\n",
        "            without_macro = f\"{model_type}_without_macro\"\n",
        "\n",
        "            if with_macro in perf_metrics.index and without_macro in perf_metrics.index:\n",
        "                sharpe_diff = perf_metrics.loc[with_macro, 'Sharpe Ratio'] - perf_metrics.loc[without_macro, 'Sharpe Ratio']\n",
        "                return_diff = perf_metrics.loc[with_macro, 'Annual Return'] - perf_metrics.loc[without_macro, 'Annual Return']\n",
        "                drawdown_diff = perf_metrics.loc[with_macro, 'Max Drawdown'] - perf_metrics.loc[without_macro, 'Max Drawdown']\n",
        "\n",
        "                key = f\"{country}_{model_type}\"\n",
        "                macro_value_analysis[key] = {\n",
        "                    'Country': country,\n",
        "                    'Model': model_type,\n",
        "                    'Sharpe Ratio Improvement': sharpe_diff,\n",
        "                    'Return Improvement': return_diff,\n",
        "                    'Drawdown Improvement': drawdown_diff,\n",
        "                    'Value Added': 'Positive' if sharpe_diff > 0 else 'Negative'\n",
        "                }\n",
        "\n",
        "    return pd.DataFrame(macro_value_analysis).T\n",
        "\n",
        "# 11. REGIME TRANSITION ANALYSIS\n",
        "def analyze_regime_transitions(simulation_results):\n",
        "    \"\"\"Analyze regime transitions and their impact on performance\"\"\"\n",
        "    transition_analysis = {}\n",
        "\n",
        "    for country, results in simulation_results.items():\n",
        "        data = results['data_with_macro']\n",
        "\n",
        "        # Identify regime transitions\n",
        "        data['regime_shift'] = data['regime'].diff().abs() > 0\n",
        "\n",
        "        # Calculate returns around transitions\n",
        "        pre_transition_returns = []\n",
        "        post_transition_returns = []\n",
        "\n",
        "        transition_indices = data[data['regime_shift']].index.tolist()\n",
        "\n",
        "        for idx in transition_indices:\n",
        "            if idx > 5 and idx < len(data) - 5:\n",
        "                # Use quarterly returns if available, otherwise monthly\n",
        "                if 'return_1q' in data.columns and data['return_1q'].notna().sum() > len(data) * 0.5:\n",
        "                    pre_return = data.iloc[idx-5:idx]['return_1q'].mean()\n",
        "                    post_return = data.iloc[idx:idx+5]['return_1q'].mean()\n",
        "                else:\n",
        "                    pre_return = data.iloc[idx-5:idx]['return_1m'].mean()\n",
        "                    post_return = data.iloc[idx:idx+5]['return_1m'].mean()\n",
        "\n",
        "                if not np.isnan(pre_return) and not np.isnan(post_return):\n",
        "                    pre_transition_returns.append(pre_return)\n",
        "                    post_transition_returns.append(post_return)\n",
        "\n",
        "        # Calculate statistics\n",
        "        if pre_transition_returns and post_transition_returns:\n",
        "            transition_analysis[country] = {\n",
        "                'Number of Transitions': len(transition_indices),\n",
        "                'Avg Pre-Transition Return': np.mean(pre_transition_returns),\n",
        "                'Avg Post-Transition Return': np.mean(post_transition_returns),\n",
        "                'Return Differential': np.mean(post_transition_returns) - np.mean(pre_transition_returns)\n",
        "            }\n",
        "        else:\n",
        "            transition_analysis[country] = {\n",
        "                'Number of Transitions': len(transition_indices),\n",
        "                'Avg Pre-Transition Return': np.nan,\n",
        "                'Avg Post-Transition Return': np.nan,\n",
        "                'Return Differential': np.nan\n",
        "            }\n",
        "\n",
        "    return transition_analysis\n",
        "\n",
        "# 12. MAIN ANALYSIS FUNCTION\n",
        "def run_market_regime_analysis(countries=['brazil', 'india', 'south_africa']):\n",
        "    \"\"\"Run market regime analysis for multiple countries\"\"\"\n",
        "    simulation_results = {}\n",
        "\n",
        "    for country in countries:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"ANALYZING {country.upper()}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        try:\n",
        "            # Load and process data\n",
        "            stock_df, macro_df = load_country_data(country)\n",
        "            if stock_df is None or macro_df is None:\n",
        "                print(f\"Skipping {country} due to data loading errors\")\n",
        "                continue\n",
        "\n",
        "            # Process data with and without macro factors\n",
        "            data_with_macro = preprocess_country_data(stock_df, macro_df, country, include_macro=True)\n",
        "            data_without_macro = preprocess_country_data(stock_df, macro_df, country, include_macro=False)\n",
        "\n",
        "            # Define market regimes\n",
        "            data_with_macro = define_market_regime(data_with_macro, method='adaptive')\n",
        "            data_without_macro = define_market_regime(data_without_macro, method='adaptive')\n",
        "\n",
        "            # Prepare features\n",
        "            X_with_macro, y_with_macro, features_with_macro = prepare_features(data_with_macro, include_macro=True)\n",
        "            X_without_macro, y_without_macro, features_without_macro = prepare_features(data_without_macro, include_macro=False)\n",
        "\n",
        "            print(f\"Features with macro: {features_with_macro}\")\n",
        "            print(f\"Features without macro: {features_without_macro}\")\n",
        "\n",
        "            # Split data using stratified time series split\n",
        "            def get_stratified_time_split(X, y, test_size=0.3):\n",
        "                # Check if we have enough data\n",
        "                if len(X) < 30:\n",
        "                    print(\"WARNING: Not enough data for proper time series split\")\n",
        "                    return train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n",
        "\n",
        "                # Split by time\n",
        "                train_size = int(len(X) * (1 - test_size))\n",
        "                X_train_time, X_test_time = X.iloc[:train_size], X.iloc[train_size:]\n",
        "                y_train_time, y_test_time = y.iloc[:train_size], y.iloc[train_size:]\n",
        "\n",
        "                # Check if all classes are in training set\n",
        "                if len(np.unique(y_train_time)) < 3:\n",
        "                    print(\"WARNING: Not all classes in time-based training split. Using stratified split instead.\")\n",
        "                    return train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n",
        "\n",
        "                return X_train_time, X_test_time, y_train_time, y_test_time\n",
        "\n",
        "            # Split data\n",
        "            X_train_with_macro, X_test_with_macro, y_train_with_macro, y_test_with_macro = get_stratified_time_split(\n",
        "                X_with_macro, y_with_macro)\n",
        "\n",
        "            X_train_without_macro, X_test_without_macro, y_train_without_macro, y_test_without_macro = get_stratified_time_split(\n",
        "                X_without_macro, y_without_macro)\n",
        "\n",
        "            # Check class distribution\n",
        "            print(f\"Training set with macro class distribution: {dict(y_train_with_macro.value_counts())}\")\n",
        "            print(f\"Test set with macro class distribution: {dict(y_test_with_macro.value_counts())}\")\n",
        "            print(f\"Training set without macro class distribution: {dict(y_train_without_macro.value_counts())}\")\n",
        "            print(f\"Test set without macro class distribution: {dict(y_test_without_macro.value_counts())}\")\n",
        "\n",
        "            # Initialize results storage\n",
        "            model_results = {'with_macro': {}, 'without_macro': {}}\n",
        "\n",
        "            # Train traditional models\n",
        "            print(\"\\nTraining traditional models WITH macroeconomic factors:\")\n",
        "            model_results['with_macro'].update(\n",
        "                train_traditional_models(\n",
        "                    X_train_with_macro, X_test_with_macro,\n",
        "                    y_train_with_macro, y_test_with_macro,\n",
        "                    features_with_macro)\n",
        "            )\n",
        "\n",
        "            print(\"\\nTraining traditional models WITHOUT macroeconomic factors:\")\n",
        "            model_results['without_macro'].update(\n",
        "                train_traditional_models(\n",
        "                    X_train_without_macro, X_test_without_macro,\n",
        "                    y_train_without_macro, y_test_without_macro,\n",
        "                    features_without_macro)\n",
        "            )\n",
        "\n",
        "            # Train deep learning models\n",
        "            print(\"\\nTraining deep learning models WITH macroeconomic factors:\")\n",
        "            model_results['with_macro'].update(\n",
        "                train_deep_learning_models(\n",
        "                    X_train_with_macro, X_test_with_macro,\n",
        "                    y_train_with_macro, y_test_with_macro,\n",
        "                    features_with_macro)\n",
        "            )\n",
        "\n",
        "            print(\"\\nTraining deep learning models WITHOUT macroeconomic factors:\")\n",
        "            model_results['without_macro'].update(\n",
        "                train_deep_learning_models(\n",
        "                    X_train_without_macro, X_test_without_macro,\n",
        "                    y_train_without_macro, y_test_without_macro,\n",
        "                    features_without_macro)\n",
        "            )\n",
        "\n",
        "            # Portfolio simulation\n",
        "            print(\"\\nSimulating portfolio performance...\")\n",
        "\n",
        "            # Collect predictions\n",
        "            predictions = {}\n",
        "            for macro_status, results in model_results.items():\n",
        "                for model_name, model_result in results.items():\n",
        "                    if 'y_pred' in model_result:\n",
        "                        key = f\"{model_name}_{macro_status}\"\n",
        "                        predictions[key] = model_result['y_pred']\n",
        "\n",
        "            # Simulation period\n",
        "            start_date = data_with_macro.iloc[len(X_train_with_macro)]['date']\n",
        "            end_date = data_with_macro.iloc[-1]['date']\n",
        "\n",
        "            # Run portfolio simulation\n",
        "            portfolio_returns = simulate_portfolio(\n",
        "                data_with_macro, predictions, start_date, end_date)\n",
        "\n",
        "            # Evaluate portfolio performance\n",
        "            performance_metrics = evaluate_portfolio_performance(portfolio_returns)\n",
        "\n",
        "            # Store country results\n",
        "            simulation_results[country] = {\n",
        "                'model_results': model_results,\n",
        "                'portfolio_returns': portfolio_returns,\n",
        "                'performance_metrics': performance_metrics,\n",
        "                'features_with_macro': features_with_macro,\n",
        "                'features_without_macro': features_without_macro,\n",
        "                'data_with_macro': data_with_macro,\n",
        "                'data_without_macro': data_without_macro\n",
        "            }\n",
        "\n",
        "            # Display performance metrics\n",
        "            print(\"\\nPortfolio Performance Metrics:\")\n",
        "            print(performance_metrics)\n",
        "\n",
        "            # Create performance comparison table\n",
        "            performance_table = pd.DataFrame({\n",
        "                'Model': list(model_results['with_macro'].keys()) + list(model_results['without_macro'].keys()),\n",
        "                'Accuracy': [model_results['with_macro'][m]['test_metrics']['accuracy'] if m in model_results['with_macro'] else np.nan for m in model_results['with_macro']] +\n",
        "                           [model_results['without_macro'][m]['test_metrics']['accuracy'] if m in model_results['without_macro'] else np.nan for m in model_results['without_macro']],\n",
        "                'F1 Score': [model_results['with_macro'][m]['test_metrics']['f1_score'] if m in model_results['with_macro'] else np.nan for m in model_results['with_macro']] +\n",
        "                           [model_results['without_macro'][m]['test_metrics']['f1_score'] if m in model_results['without_macro'] else np.nan for m in model_results['without_macro']],\n",
        "                'Training Time (s)': [model_results['with_macro'][m]['training_time'] if m in model_results['with_macro'] else np.nan for m in model_results['with_macro']] +\n",
        "                                    [model_results['without_macro'][m]['training_time'] if m in model_results['without_macro'] else np.nan for m in model_results['without_macro']],\n",
        "                'Memory Usage (%)': [model_results['with_macro'][m]['memory_usage'] if m in model_results['with_macro'] else np.nan for m in model_results['with_macro']] +\n",
        "                                   [model_results['without_macro'][m]['memory_usage'] if m in model_results['without_macro'] else np.nan for m in model_results['without_macro']]\n",
        "            })\n",
        "\n",
        "            print(\"\\nModel Performance Comparison:\")\n",
        "            print(performance_table)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing {country}: {str(e)}\")\n",
        "\n",
        "    # Cross-country analysis\n",
        "    if simulation_results:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"CROSS-COUNTRY ANALYSIS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Compare performance across countries\n",
        "        performance_comparison = {}\n",
        "\n",
        "        for country, results in simulation_results.items():\n",
        "            if 'performance_metrics' in results:\n",
        "                perf_metrics = results['performance_metrics']\n",
        "\n",
        "                for strategy in perf_metrics.index:\n",
        "                    if strategy not in performance_comparison:\n",
        "                        performance_comparison[strategy] = {}\n",
        "\n",
        "                    performance_comparison[strategy][country] = {\n",
        "                        'Sharpe Ratio': perf_metrics.loc[strategy, 'Sharpe Ratio'],\n",
        "                        'Annual Return': perf_metrics.loc[strategy, 'Annual Return'],\n",
        "                        'Max Drawdown': perf_metrics.loc[strategy, 'Max Drawdown']\n",
        "                    }\n",
        "\n",
        "        # Display cross-country comparison\n",
        "        for strategy, country_perf in performance_comparison.items():\n",
        "            print(f\"\\n{strategy} Performance Across Countries:\")\n",
        "            comparison_df = pd.DataFrame(country_perf).T\n",
        "            print(comparison_df)\n",
        "\n",
        "        # Analyze value added by macroeconomic factors\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"VALUE ADDED BY MACROECONOMIC FACTORS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        macro_value_analysis = analyze_macro_value_added(simulation_results)\n",
        "        print(\"\\nValue Added by Macroeconomic Factors:\")\n",
        "        print(macro_value_analysis)\n",
        "\n",
        "        # Analyze regime transitions\n",
        "        transition_analysis = analyze_regime_transitions(simulation_results)\n",
        "        print(\"\\nRegime Transition Analysis:\")\n",
        "        for country, analysis in transition_analysis.items():\n",
        "            print(f\"\\n{country.upper()}:\")\n",
        "            for metric, value in analysis.items():\n",
        "                print(f\"  {metric}: {value:.4f}\" if isinstance(value, float) else f\"  {metric}: {value}\")\n",
        "\n",
        "    return simulation_results\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_market_regime_analysis()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huf-y0EL_IqK",
        "outputId": "61c648d2-d2f5-46a2-894a-4fed1c44a12d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "==================================================\n",
            "ANALYZING BRAZIL\n",
            "==================================================\n",
            "Loaded brazil data: 148 stock rows, 52 macro rows\n",
            "Available macro columns for brazil: ['imports', 'exports', 'gdp', 'interest_rate', 'cpi', 'exchange_rate']\n",
            "Successfully processed brazil data: 148 rows, 27 columns\n",
            "Successfully processed brazil data: 148 rows, 15 columns\n",
            "Quarterly metrics not available, using monthly metrics\n",
            "Regime distribution: {1: np.int64(96), 2: np.int64(45), 0: np.int64(7)}\n",
            "Final regime distribution: {1: np.int64(96), 2: np.int64(45), 0: np.int64(7)}\n",
            "Quarterly metrics not available, using monthly metrics\n",
            "Regime distribution: {1: np.int64(96), 2: np.int64(45), 0: np.int64(7)}\n",
            "Final regime distribution: {1: np.int64(96), 2: np.int64(45), 0: np.int64(7)}\n",
            "Removing completely empty features: ['volatility_2q']\n",
            "Removing completely empty features: ['volatility_2q']\n",
            "Features with macro: ['return_1q', 'return_2q', 'ma_ratio', 'momentum', 'gdp', 'cpi', 'exchange_rate', 'imports', 'exports', 'gdp_change', 'cpi_change', 'imports_change', 'exports_change', 'interest_rate', 'interest_rate_change']\n",
            "Features without macro: ['return_1q', 'return_2q', 'ma_ratio', 'momentum']\n",
            "WARNING: Not all classes in time-based training split. Using stratified split instead.\n",
            "WARNING: Not all classes in time-based training split. Using stratified split instead.\n",
            "Training set with macro class distribution: {1: np.int64(67), 2: np.int64(31), 0: np.int64(5)}\n",
            "Test set with macro class distribution: {1: np.int64(29), 2: np.int64(14), 0: np.int64(2)}\n",
            "Training set without macro class distribution: {1: np.int64(67), 2: np.int64(31), 0: np.int64(5)}\n",
            "Test set without macro class distribution: {1: np.int64(29), 2: np.int64(14), 0: np.int64(2)}\n",
            "\n",
            "Training traditional models WITH macroeconomic factors:\n",
            "Training RandomForest...\n",
            "RandomForest - Train Accuracy: 1.0000, Test Accuracy: 0.9111\n",
            "RandomForest - Train Balanced Acc: 1.0000, Test Balanced Acc: 0.9294\n",
            "RandomForest - Training Time: 0.95s, Memory Usage: 0.10%\n",
            "Training SVC...\n",
            "SVC - Train Accuracy: 0.9029, Test Accuracy: 0.8222\n",
            "SVC - Train Balanced Acc: 0.9329, Test Balanced Acc: 0.8588\n",
            "SVC - Training Time: 0.05s, Memory Usage: 0.00%\n",
            "Training XGBoost...\n",
            "XGBoost - Train Accuracy: 1.0000, Test Accuracy: 0.8889\n",
            "XGBoost - Train Balanced Acc: 1.0000, Test Balanced Acc: 0.9302\n",
            "XGBoost - Training Time: 1.03s, Memory Usage: 0.00%\n",
            "\n",
            "Training traditional models WITHOUT macroeconomic factors:\n",
            "Training RandomForest...\n",
            "RandomForest - Train Accuracy: 1.0000, Test Accuracy: 0.8667\n",
            "RandomForest - Train Balanced Acc: 1.0000, Test Balanced Acc: 0.6084\n",
            "RandomForest - Training Time: 0.97s, Memory Usage: 0.40%\n",
            "Training SVC...\n",
            "SVC - Train Accuracy: 0.7476, Test Accuracy: 0.6444\n",
            "SVC - Train Balanced Acc: 0.8591, Test Balanced Acc: 0.8038\n",
            "SVC - Training Time: 0.08s, Memory Usage: 0.00%\n",
            "Training XGBoost...\n",
            "XGBoost - Train Accuracy: 1.0000, Test Accuracy: 0.8222\n",
            "XGBoost - Train Balanced Acc: 1.0000, Test Balanced Acc: 0.5731\n",
            "XGBoost - Training Time: 2.53s, Memory Usage: 0.10%\n",
            "\n",
            "Training deep learning models WITH macroeconomic factors:\n",
            "Training LSTM...\n",
            "Epoch 1/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 206ms/step - accuracy: 0.3735 - loss: 1.1043 - val_accuracy: 0.3684 - val_loss: 1.0979\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.4689 - loss: 1.0722 - val_accuracy: 0.4737 - val_loss: 1.0719\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5499 - loss: 1.0397 - val_accuracy: 0.6842 - val_loss: 1.0411\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6360 - loss: 1.0004 - val_accuracy: 0.7895 - val_loss: 1.0070\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5318 - loss: 0.9864 - val_accuracy: 0.8421 - val_loss: 0.9719\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6262 - loss: 0.9201 - val_accuracy: 0.8421 - val_loss: 0.9201\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5792 - loss: 0.9056 - val_accuracy: 0.8421 - val_loss: 0.8558\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6114 - loss: 0.8860 - val_accuracy: 0.8421 - val_loss: 0.7970\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5861 - loss: 0.8293 - val_accuracy: 0.8421 - val_loss: 0.7510\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5712 - loss: 0.8376 - val_accuracy: 0.8421 - val_loss: 0.7208\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5882 - loss: 0.8156 - val_accuracy: 0.8421 - val_loss: 0.6913\n",
            "Epoch 12/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5705 - loss: 0.7732 - val_accuracy: 0.8421 - val_loss: 0.6688\n",
            "Epoch 13/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.6290 - loss: 0.8568 - val_accuracy: 0.7895 - val_loss: 0.6792\n",
            "Epoch 14/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6526 - loss: 0.7678 - val_accuracy: 0.7895 - val_loss: 0.7003\n",
            "Epoch 15/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.6095 - loss: 0.7812 - val_accuracy: 0.7368 - val_loss: 0.7542\n",
            "Epoch 16/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6939 - loss: 0.7445 - val_accuracy: 0.6842 - val_loss: 0.7942\n",
            "Epoch 17/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6077 - loss: 0.8205 - val_accuracy: 0.6842 - val_loss: 0.8220\n",
            "Epoch 18/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6037 - loss: 0.8096 - val_accuracy: 0.6316 - val_loss: 0.8175\n",
            "Epoch 19/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6298 - loss: 0.7886 - val_accuracy: 0.6316 - val_loss: 0.8190\n",
            "Epoch 20/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6771 - loss: 0.6914 - val_accuracy: 0.6316 - val_loss: 0.8095\n",
            "Epoch 21/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6746 - loss: 0.7652 - val_accuracy: 0.6316 - val_loss: 0.8034\n",
            "Epoch 22/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.6257 - loss: 0.7902 - val_accuracy: 0.6316 - val_loss: 0.8175\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 170ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "LSTM - Train Accuracy: 0.6559, Test Accuracy: 0.6000\n",
            "LSTM - Train Balanced Acc: 0.3622, Test Balanced Acc: 0.3333\n",
            "LSTM - Training Time: 16.43s, Memory Usage: 0.30%\n",
            "Training GRU...\n",
            "Epoch 1/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 213ms/step - accuracy: 0.3905 - loss: 1.0945 - val_accuracy: 0.6842 - val_loss: 0.9987\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5152 - loss: 1.0021 - val_accuracy: 0.7368 - val_loss: 0.9306\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5781 - loss: 0.9833 - val_accuracy: 0.7895 - val_loss: 0.8727\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5502 - loss: 0.9345 - val_accuracy: 0.7895 - val_loss: 0.8108\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6573 - loss: 0.8711 - val_accuracy: 0.7895 - val_loss: 0.7539\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5936 - loss: 0.8784 - val_accuracy: 0.7895 - val_loss: 0.7049\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5592 - loss: 0.8779 - val_accuracy: 0.7895 - val_loss: 0.6612\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6139 - loss: 0.8276 - val_accuracy: 0.7895 - val_loss: 0.6223\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5857 - loss: 0.8532 - val_accuracy: 0.7895 - val_loss: 0.6007\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.6316 - loss: 0.8047 - val_accuracy: 0.7895 - val_loss: 0.5958\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6595 - loss: 0.7816 - val_accuracy: 0.7895 - val_loss: 0.5909\n",
            "Epoch 12/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6917 - loss: 0.7380 - val_accuracy: 0.7895 - val_loss: 0.5946\n",
            "Epoch 13/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5971 - loss: 0.8316 - val_accuracy: 0.7895 - val_loss: 0.6175\n",
            "Epoch 14/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6746 - loss: 0.7494 - val_accuracy: 0.7895 - val_loss: 0.6304\n",
            "Epoch 15/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6902 - loss: 0.7708 - val_accuracy: 0.7368 - val_loss: 0.6441\n",
            "Epoch 16/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6721 - loss: 0.7373 - val_accuracy: 0.7368 - val_loss: 0.6316\n",
            "Epoch 17/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6460 - loss: 0.7249 - val_accuracy: 0.7895 - val_loss: 0.5943\n",
            "Epoch 18/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6761 - loss: 0.7027 - val_accuracy: 0.7895 - val_loss: 0.5607\n",
            "Epoch 19/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7014 - loss: 0.7223 - val_accuracy: 0.7895 - val_loss: 0.5403\n",
            "Epoch 20/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7195 - loss: 0.7161 - val_accuracy: 0.7895 - val_loss: 0.5304\n",
            "Epoch 21/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6993 - loss: 0.7050 - val_accuracy: 0.7895 - val_loss: 0.5324\n",
            "Epoch 22/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6684 - loss: 0.7115 - val_accuracy: 0.7895 - val_loss: 0.5550\n",
            "Epoch 23/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.6580 - loss: 0.7056 - val_accuracy: 0.7895 - val_loss: 0.5679\n",
            "Epoch 24/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.6421 - loss: 0.6922 - val_accuracy: 0.7368 - val_loss: 0.5896\n",
            "Epoch 25/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7596 - loss: 0.6610 - val_accuracy: 0.7368 - val_loss: 0.6084\n",
            "Epoch 26/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6742 - loss: 0.6739 - val_accuracy: 0.7368 - val_loss: 0.6005\n",
            "Epoch 27/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7148 - loss: 0.6529 - val_accuracy: 0.7368 - val_loss: 0.5739\n",
            "Epoch 28/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.6662 - loss: 0.6857 - val_accuracy: 0.7895 - val_loss: 0.5576\n",
            "Epoch 29/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6435 - loss: 0.6934 - val_accuracy: 0.7895 - val_loss: 0.5358\n",
            "Epoch 30/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7335 - loss: 0.6314 - val_accuracy: 0.7895 - val_loss: 0.5272\n",
            "Epoch 31/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7731 - loss: 0.5823 - val_accuracy: 0.7895 - val_loss: 0.5167\n",
            "Epoch 32/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7057 - loss: 0.5763 - val_accuracy: 0.7895 - val_loss: 0.5224\n",
            "Epoch 33/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7006 - loss: 0.6245 - val_accuracy: 0.7895 - val_loss: 0.5463\n",
            "Epoch 34/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7412 - loss: 0.5765 - val_accuracy: 0.7368 - val_loss: 0.5593\n",
            "Epoch 35/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6902 - loss: 0.6195 - val_accuracy: 0.7368 - val_loss: 0.5633\n",
            "Epoch 36/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7559 - loss: 0.5815 - val_accuracy: 0.7368 - val_loss: 0.5615\n",
            "Epoch 37/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7841 - loss: 0.5574 - val_accuracy: 0.7368 - val_loss: 0.5617\n",
            "Epoch 38/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7845 - loss: 0.5598 - val_accuracy: 0.7368 - val_loss: 0.5622\n",
            "Epoch 39/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7592 - loss: 0.5580 - val_accuracy: 0.7368 - val_loss: 0.5486\n",
            "Epoch 40/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7024 - loss: 0.5419 - val_accuracy: 0.7368 - val_loss: 0.5584\n",
            "Epoch 41/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7570 - loss: 0.5521 - val_accuracy: 0.7368 - val_loss: 0.5827\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 191ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "GRU - Train Accuracy: 0.7204, Test Accuracy: 0.5429\n",
            "GRU - Train Balanced Acc: 0.4980, Test Balanced Acc: 0.3016\n",
            "GRU - Training Time: 16.01s, Memory Usage: 0.30%\n",
            "\n",
            "Training deep learning models WITHOUT macroeconomic factors:\n",
            "Training LSTM...\n",
            "Epoch 1/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 201ms/step - accuracy: 0.3640 - loss: 1.0998 - val_accuracy: 0.6316 - val_loss: 1.0624\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.4963 - loss: 1.0723 - val_accuracy: 0.8421 - val_loss: 1.0379\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5930 - loss: 1.0546 - val_accuracy: 0.8421 - val_loss: 1.0081\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6005 - loss: 1.0305 - val_accuracy: 0.8421 - val_loss: 0.9737\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6038 - loss: 0.9914 - val_accuracy: 0.8421 - val_loss: 0.9293\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5451 - loss: 0.9917 - val_accuracy: 0.8421 - val_loss: 0.8759\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5579 - loss: 0.9336 - val_accuracy: 0.8421 - val_loss: 0.8042\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6038 - loss: 0.9084 - val_accuracy: 0.8421 - val_loss: 0.7315\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6078 - loss: 0.8607 - val_accuracy: 0.8421 - val_loss: 0.6758\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5846 - loss: 0.8697 - val_accuracy: 0.8421 - val_loss: 0.6440\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5672 - loss: 0.8992 - val_accuracy: 0.8421 - val_loss: 0.6251\n",
            "Epoch 12/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5629 - loss: 0.8420 - val_accuracy: 0.8421 - val_loss: 0.6270\n",
            "Epoch 13/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5991 - loss: 0.8082 - val_accuracy: 0.8421 - val_loss: 0.6051\n",
            "Epoch 14/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6215 - loss: 0.7929 - val_accuracy: 0.8421 - val_loss: 0.5754\n",
            "Epoch 15/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5948 - loss: 0.8432 - val_accuracy: 0.8421 - val_loss: 0.5633\n",
            "Epoch 16/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6146 - loss: 0.7725 - val_accuracy: 0.8421 - val_loss: 0.5659\n",
            "Epoch 17/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.6791 - loss: 0.7926 - val_accuracy: 0.8421 - val_loss: 0.5688\n",
            "Epoch 18/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5293 - loss: 0.8681 - val_accuracy: 0.8421 - val_loss: 0.5952\n",
            "Epoch 19/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6201 - loss: 0.8111 - val_accuracy: 0.8421 - val_loss: 0.6076\n",
            "Epoch 20/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6038 - loss: 0.7940 - val_accuracy: 0.8421 - val_loss: 0.5941\n",
            "Epoch 21/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6480 - loss: 0.7221 - val_accuracy: 0.8421 - val_loss: 0.5886\n",
            "Epoch 22/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5799 - loss: 0.8335 - val_accuracy: 0.8421 - val_loss: 0.5836\n",
            "Epoch 23/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5242 - loss: 0.8292 - val_accuracy: 0.8421 - val_loss: 0.5645\n",
            "Epoch 24/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6016 - loss: 0.7888 - val_accuracy: 0.8421 - val_loss: 0.5511\n",
            "Epoch 25/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6182 - loss: 0.8014 - val_accuracy: 0.8421 - val_loss: 0.5278\n",
            "Epoch 26/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5415 - loss: 0.8591 - val_accuracy: 0.8421 - val_loss: 0.5091\n",
            "Epoch 27/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5477 - loss: 0.8487 - val_accuracy: 0.8421 - val_loss: 0.4916\n",
            "Epoch 28/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6060 - loss: 0.7824 - val_accuracy: 0.8421 - val_loss: 0.4890\n",
            "Epoch 29/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6099 - loss: 0.7834 - val_accuracy: 0.8421 - val_loss: 0.4902\n",
            "Epoch 30/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6103 - loss: 0.7920 - val_accuracy: 0.8421 - val_loss: 0.4883\n",
            "Epoch 31/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6255 - loss: 0.7590 - val_accuracy: 0.8421 - val_loss: 0.5034\n",
            "Epoch 32/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5995 - loss: 0.7519 - val_accuracy: 0.8947 - val_loss: 0.5287\n",
            "Epoch 33/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5994 - loss: 0.7766 - val_accuracy: 0.8947 - val_loss: 0.5391\n",
            "Epoch 34/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5409 - loss: 0.7652 - val_accuracy: 0.8947 - val_loss: 0.5348\n",
            "Epoch 35/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6381 - loss: 0.7636 - val_accuracy: 0.8947 - val_loss: 0.5245\n",
            "Epoch 36/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7010 - loss: 0.7771 - val_accuracy: 0.8947 - val_loss: 0.4967\n",
            "Epoch 37/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5792 - loss: 0.7268 - val_accuracy: 0.8947 - val_loss: 0.4604\n",
            "Epoch 38/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6179 - loss: 0.7095 - val_accuracy: 0.8947 - val_loss: 0.4420\n",
            "Epoch 39/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.6280 - loss: 0.7556 - val_accuracy: 0.8947 - val_loss: 0.4432\n",
            "Epoch 40/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6348 - loss: 0.7345 - val_accuracy: 0.8947 - val_loss: 0.4622\n",
            "Epoch 41/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6617 - loss: 0.7617 - val_accuracy: 0.8947 - val_loss: 0.4775\n",
            "Epoch 42/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6794 - loss: 0.7543 - val_accuracy: 0.9474 - val_loss: 0.5175\n",
            "Epoch 43/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.6255 - loss: 0.7202 - val_accuracy: 0.8947 - val_loss: 0.5268\n",
            "Epoch 44/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6414 - loss: 0.6848 - val_accuracy: 0.8947 - val_loss: 0.5080\n",
            "Epoch 45/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5969 - loss: 0.7004 - val_accuracy: 0.8947 - val_loss: 0.4692\n",
            "Epoch 46/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6794 - loss: 0.6463 - val_accuracy: 0.8947 - val_loss: 0.4354\n",
            "Epoch 47/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6667 - loss: 0.6842 - val_accuracy: 0.8947 - val_loss: 0.4304\n",
            "Epoch 48/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6315 - loss: 0.7304 - val_accuracy: 0.8421 - val_loss: 0.4339\n",
            "Epoch 49/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.5256 - loss: 0.7733 - val_accuracy: 0.8421 - val_loss: 0.4422\n",
            "Epoch 50/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6873 - loss: 0.7047 - val_accuracy: 0.8947 - val_loss: 0.4245\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "LSTM - Train Accuracy: 0.7312, Test Accuracy: 0.5429\n",
            "LSTM - Train Balanced Acc: 0.4602, Test Balanced Acc: 0.3611\n",
            "LSTM - Training Time: 15.64s, Memory Usage: 0.60%\n",
            "Training GRU...\n",
            "Epoch 1/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 309ms/step - accuracy: 0.3889 - loss: 1.0866 - val_accuracy: 0.3158 - val_loss: 1.0885\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5419 - loss: 1.0532 - val_accuracy: 0.5263 - val_loss: 1.0421\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.4996 - loss: 1.0159 - val_accuracy: 0.5263 - val_loss: 0.9967\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5974 - loss: 0.9840 - val_accuracy: 0.5789 - val_loss: 0.9497\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5094 - loss: 0.9646 - val_accuracy: 0.6316 - val_loss: 0.8963\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6157 - loss: 0.9255 - val_accuracy: 0.6842 - val_loss: 0.8426\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5897 - loss: 0.8578 - val_accuracy: 0.6842 - val_loss: 0.7946\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5936 - loss: 0.8719 - val_accuracy: 0.7368 - val_loss: 0.7539\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5933 - loss: 0.8429 - val_accuracy: 0.7368 - val_loss: 0.7237\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6406 - loss: 0.8329 - val_accuracy: 0.8947 - val_loss: 0.6870\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5907 - loss: 0.8388 - val_accuracy: 0.8947 - val_loss: 0.6542\n",
            "Epoch 12/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6458 - loss: 0.7868 - val_accuracy: 0.8947 - val_loss: 0.6292\n",
            "Epoch 13/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5882 - loss: 0.8265 - val_accuracy: 0.8947 - val_loss: 0.6243\n",
            "Epoch 14/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6237 - loss: 0.7311 - val_accuracy: 0.8947 - val_loss: 0.6239\n",
            "Epoch 15/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5741 - loss: 0.8433 - val_accuracy: 0.8947 - val_loss: 0.6437\n",
            "Epoch 16/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5498 - loss: 0.8416 - val_accuracy: 0.8421 - val_loss: 0.6534\n",
            "Epoch 17/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5571 - loss: 0.8376 - val_accuracy: 0.8947 - val_loss: 0.6377\n",
            "Epoch 18/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6721 - loss: 0.7528 - val_accuracy: 0.8947 - val_loss: 0.6258\n",
            "Epoch 19/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6262 - loss: 0.7763 - val_accuracy: 0.8421 - val_loss: 0.6231\n",
            "Epoch 20/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6468 - loss: 0.8167 - val_accuracy: 0.8421 - val_loss: 0.6073\n",
            "Epoch 21/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6552 - loss: 0.7687 - val_accuracy: 0.8421 - val_loss: 0.5971\n",
            "Epoch 22/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6327 - loss: 0.7625 - val_accuracy: 0.8421 - val_loss: 0.6037\n",
            "Epoch 23/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6400 - loss: 0.7860 - val_accuracy: 0.8947 - val_loss: 0.6194\n",
            "Epoch 24/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5932 - loss: 0.7890 - val_accuracy: 0.8947 - val_loss: 0.6245\n",
            "Epoch 25/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6721 - loss: 0.7819 - val_accuracy: 0.8947 - val_loss: 0.6103\n",
            "Epoch 26/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6627 - loss: 0.7393 - val_accuracy: 0.8947 - val_loss: 0.5943\n",
            "Epoch 27/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6363 - loss: 0.8314 - val_accuracy: 0.8947 - val_loss: 0.5881\n",
            "Epoch 28/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6682 - loss: 0.7492 - val_accuracy: 0.8947 - val_loss: 0.5745\n",
            "Epoch 29/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6305 - loss: 0.8275 - val_accuracy: 0.8947 - val_loss: 0.5726\n",
            "Epoch 30/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6342 - loss: 0.7981 - val_accuracy: 0.8421 - val_loss: 0.5590\n",
            "Epoch 31/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6029 - loss: 0.7568 - val_accuracy: 0.8947 - val_loss: 0.5695\n",
            "Epoch 32/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6269 - loss: 0.7620 - val_accuracy: 0.8947 - val_loss: 0.5817\n",
            "Epoch 33/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6758 - loss: 0.7329 - val_accuracy: 0.8947 - val_loss: 0.5853\n",
            "Epoch 34/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6703 - loss: 0.7076 - val_accuracy: 0.8947 - val_loss: 0.5978\n",
            "Epoch 35/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5436 - loss: 0.7798 - val_accuracy: 0.8947 - val_loss: 0.5951\n",
            "Epoch 36/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6725 - loss: 0.7054 - val_accuracy: 0.8947 - val_loss: 0.5965\n",
            "Epoch 37/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6858 - loss: 0.7087 - val_accuracy: 0.8947 - val_loss: 0.5970\n",
            "Epoch 38/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6837 - loss: 0.7145 - val_accuracy: 0.8947 - val_loss: 0.5817\n",
            "Epoch 39/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6591 - loss: 0.7104 - val_accuracy: 0.8947 - val_loss: 0.5951\n",
            "Epoch 40/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6286 - loss: 0.7892 - val_accuracy: 0.8947 - val_loss: 0.6081\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "GRU - Train Accuracy: 0.6989, Test Accuracy: 0.6000\n",
            "GRU - Train Balanced Acc: 0.4633, Test Balanced Acc: 0.3690\n",
            "GRU - Training Time: 14.84s, Memory Usage: 0.40%\n",
            "\n",
            "Simulating portfolio performance...\n",
            "\n",
            "Portfolio Performance Metrics:\n",
            "                            Total Return  Annual Return  Volatility  \\\n",
            "Buy and Hold                    0.168239       1.436538    0.809226   \n",
            "RandomForest_with_macro         0.032241       0.199301    0.402591   \n",
            "SVC_with_macro                 -0.025457      -0.137300    0.409675   \n",
            "XGBoost_with_macro              0.094738       0.679348    0.413825   \n",
            "LSTM_with_macro                 0.000000       0.000000    0.000000   \n",
            "GRU_with_macro                  0.031930       0.253956    0.214951   \n",
            "RandomForest_without_macro      0.134833       1.063526    0.332781   \n",
            "SVC_without_macro              -0.043318      -0.224018    0.619251   \n",
            "XGBoost_without_macro           0.141304       1.131831    0.332743   \n",
            "LSTM_without_macro              0.260762       4.303455    0.545413   \n",
            "GRU_without_macro               0.076981       0.705680    0.407916   \n",
            "\n",
            "                            Sharpe Ratio  Max Drawdown  Win Rate  \n",
            "Buy and Hold                    1.775199     -0.178810  0.522727  \n",
            "RandomForest_with_macro         0.495046     -0.146547  0.204545  \n",
            "SVC_with_macro                 -0.335144     -0.211786  0.204545  \n",
            "XGBoost_with_macro              1.641633     -0.119560  0.250000  \n",
            "LSTM_with_macro                 0.000000      0.000000  0.000000  \n",
            "GRU_with_macro                  1.181460     -0.048893  0.057143  \n",
            "RandomForest_without_macro      3.195870     -0.093285  0.204545  \n",
            "SVC_without_macro              -0.361757     -0.181708  0.363636  \n",
            "XGBoost_without_macro           3.401519     -0.093285  0.227273  \n",
            "LSTM_without_macro              7.890268     -0.068315  0.228571  \n",
            "GRU_without_macro               1.729966     -0.090987  0.085714  \n",
            "\n",
            "Model Performance Comparison:\n",
            "          Model  Accuracy  F1 Score  Training Time (s)  Memory Usage (%)\n",
            "0  RandomForest  0.911111  0.912099           0.946951               0.1\n",
            "1           SVC  0.822222  0.830015           0.046548               0.0\n",
            "2       XGBoost  0.888889  0.894415           1.033006               0.0\n",
            "3          LSTM  0.600000  0.450000          16.428939               0.3\n",
            "4           GRU  0.542857  0.430189          16.005780               0.3\n",
            "5  RandomForest  0.866667  0.856705           0.967155               0.4\n",
            "6           SVC  0.644444  0.675577           0.077146               0.0\n",
            "7       XGBoost  0.822222  0.813027           2.529036               0.1\n",
            "8          LSTM  0.542857  0.531868          15.642318               0.6\n",
            "9           GRU  0.600000  0.549087          14.844080               0.4\n",
            "\n",
            "==================================================\n",
            "ANALYZING INDIA\n",
            "==================================================\n",
            "Loaded india data: 148 stock rows, 52 macro rows\n",
            "Available macro columns for india: ['imports', 'exports', 'interest_rate', 'gdp', 'exchange_rate', 'cpi']\n",
            "Successfully processed india data: 148 rows, 27 columns\n",
            "Successfully processed india data: 148 rows, 15 columns\n",
            "Quarterly metrics not available, using monthly metrics\n",
            "Regime distribution: {1: np.int64(118), 2: np.int64(29), 0: np.int64(1)}\n",
            "Final regime distribution: {1: np.int64(118), 2: np.int64(29), 0: np.int64(1)}\n",
            "Quarterly metrics not available, using monthly metrics\n",
            "Regime distribution: {1: np.int64(118), 2: np.int64(29), 0: np.int64(1)}\n",
            "Final regime distribution: {1: np.int64(118), 2: np.int64(29), 0: np.int64(1)}\n",
            "Removing completely empty features: ['volatility_2q']\n",
            "Removing completely empty features: ['volatility_2q']\n",
            "Features with macro: ['return_1q', 'return_2q', 'ma_ratio', 'momentum', 'gdp', 'cpi', 'exchange_rate', 'imports', 'exports', 'gdp_change', 'cpi_change', 'imports_change', 'exports_change', 'interest_rate', 'interest_rate_change']\n",
            "Features without macro: ['return_1q', 'return_2q', 'ma_ratio', 'momentum']\n",
            "Training set with macro class distribution: {1: np.int64(90), 2: np.int64(12), 0: np.int64(1)}\n",
            "Test set with macro class distribution: {1: np.int64(28), 2: np.int64(17)}\n",
            "Training set without macro class distribution: {1: np.int64(90), 2: np.int64(12), 0: np.int64(1)}\n",
            "Test set without macro class distribution: {1: np.int64(28), 2: np.int64(17)}\n",
            "\n",
            "Training traditional models WITH macroeconomic factors:\n",
            "Training RandomForest...\n",
            "RandomForest - Train Accuracy: 1.0000, Test Accuracy: 0.6222\n",
            "RandomForest - Train Balanced Acc: 1.0000, Test Balanced Acc: 0.5000\n",
            "RandomForest - Training Time: 0.35s, Memory Usage: 0.00%\n",
            "Training SVC...\n",
            "SVC - Train Accuracy: 0.6602, Test Accuracy: 0.7556\n",
            "SVC - Train Balanced Acc: 0.8704, Test Balanced Acc: 0.6765\n",
            "SVC - Training Time: 0.03s, Memory Usage: 0.00%\n",
            "Training XGBoost...\n",
            "XGBoost - Train Accuracy: 0.9903, Test Accuracy: 0.8000\n",
            "XGBoost - Train Balanced Acc: 0.6667, Test Balanced Acc: 0.7584\n",
            "XGBoost - Training Time: 0.11s, Memory Usage: 0.00%\n",
            "\n",
            "Training traditional models WITHOUT macroeconomic factors:\n",
            "Training RandomForest...\n",
            "RandomForest - Train Accuracy: 1.0000, Test Accuracy: 0.7333\n",
            "RandomForest - Train Balanced Acc: 1.0000, Test Balanced Acc: 0.6817\n",
            "RandomForest - Training Time: 0.35s, Memory Usage: 0.00%\n",
            "Training SVC...\n",
            "SVC - Train Accuracy: 0.6214, Test Accuracy: 0.7556\n",
            "SVC - Train Balanced Acc: 0.8556, Test Balanced Acc: 0.7805\n",
            "SVC - Training Time: 0.03s, Memory Usage: 0.00%\n",
            "Training XGBoost...\n",
            "XGBoost - Train Accuracy: 0.9903, Test Accuracy: 0.8222\n",
            "XGBoost - Train Balanced Acc: 0.6667, Test Balanced Acc: 0.7878\n",
            "XGBoost - Training Time: 0.13s, Memory Usage: 0.00%\n",
            "\n",
            "Training deep learning models WITH macroeconomic factors:\n",
            "Training LSTM...\n",
            "Epoch 1/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 198ms/step - accuracy: 0.4551 - loss: 1.1077 - val_accuracy: 0.7368 - val_loss: 0.9676\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6558 - loss: 0.9809 - val_accuracy: 0.6842 - val_loss: 0.9775\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7866 - loss: 0.8932 - val_accuracy: 0.6842 - val_loss: 0.9390\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8699 - loss: 0.8022 - val_accuracy: 0.7368 - val_loss: 0.8454\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8370 - loss: 0.7205 - val_accuracy: 0.8421 - val_loss: 0.7117\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8160 - loss: 0.6520 - val_accuracy: 0.9474 - val_loss: 0.5537\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8260 - loss: 0.5735 - val_accuracy: 0.9474 - val_loss: 0.4477\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8047 - loss: 0.5510 - val_accuracy: 0.9474 - val_loss: 0.3619\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8250 - loss: 0.4677 - val_accuracy: 0.9474 - val_loss: 0.3126\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8511 - loss: 0.4543 - val_accuracy: 0.9474 - val_loss: 0.2805\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7917 - loss: 0.5325 - val_accuracy: 0.9474 - val_loss: 0.2545\n",
            "Epoch 12/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8507 - loss: 0.4371 - val_accuracy: 0.9474 - val_loss: 0.2422\n",
            "Epoch 13/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8430 - loss: 0.4707 - val_accuracy: 0.9474 - val_loss: 0.2416\n",
            "Epoch 14/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8026 - loss: 0.4315 - val_accuracy: 0.9474 - val_loss: 0.2419\n",
            "Epoch 15/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8312 - loss: 0.4263 - val_accuracy: 0.9474 - val_loss: 0.2521\n",
            "Epoch 16/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8449 - loss: 0.3877 - val_accuracy: 0.9474 - val_loss: 0.2579\n",
            "Epoch 17/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8167 - loss: 0.4481 - val_accuracy: 0.9474 - val_loss: 0.2651\n",
            "Epoch 18/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8757 - loss: 0.3599 - val_accuracy: 0.9474 - val_loss: 0.2630\n",
            "Epoch 19/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8138 - loss: 0.4042 - val_accuracy: 0.9474 - val_loss: 0.2694\n",
            "Epoch 20/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8268 - loss: 0.4074 - val_accuracy: 0.9474 - val_loss: 0.2697\n",
            "Epoch 21/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8260 - loss: 0.4261 - val_accuracy: 0.9474 - val_loss: 0.2674\n",
            "Epoch 22/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8424 - loss: 0.3782 - val_accuracy: 0.9474 - val_loss: 0.2612\n",
            "Epoch 23/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8119 - loss: 0.4303 - val_accuracy: 0.9474 - val_loss: 0.2562\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 606ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "LSTM - Train Accuracy: 0.8602, Test Accuracy: 0.8000\n",
            "LSTM - Train Balanced Acc: 0.3333, Test Balanced Acc: 0.5000\n",
            "LSTM - Training Time: 11.78s, Memory Usage: 0.40%\n",
            "Training GRU...\n",
            "Epoch 1/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 327ms/step - accuracy: 0.4190 - loss: 1.0850 - val_accuracy: 0.0000e+00 - val_loss: 1.3597\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5328 - loss: 0.9603 - val_accuracy: 0.0000e+00 - val_loss: 1.5112\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8214 - loss: 0.8164 - val_accuracy: 0.0000e+00 - val_loss: 1.6049\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8131 - loss: 0.6944 - val_accuracy: 0.0000e+00 - val_loss: 1.6506\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8142 - loss: 0.6370 - val_accuracy: 0.0000e+00 - val_loss: 1.5981\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8210 - loss: 0.5239 - val_accuracy: 0.0000e+00 - val_loss: 1.5341\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8308 - loss: 0.5300 - val_accuracy: 0.1053 - val_loss: 1.4497\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8264 - loss: 0.4886 - val_accuracy: 0.1579 - val_loss: 1.3691\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7881 - loss: 0.5255 - val_accuracy: 0.2105 - val_loss: 1.1815\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7899 - loss: 0.4373 - val_accuracy: 0.3158 - val_loss: 1.1512\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8395 - loss: 0.4422 - val_accuracy: 0.3684 - val_loss: 1.0490\n",
            "Epoch 12/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8608 - loss: 0.3642 - val_accuracy: 0.4211 - val_loss: 0.9729\n",
            "Epoch 13/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7664 - loss: 0.4798 - val_accuracy: 0.4737 - val_loss: 0.9720\n",
            "Epoch 14/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7914 - loss: 0.4548 - val_accuracy: 0.5263 - val_loss: 0.9398\n",
            "Epoch 15/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8204 - loss: 0.4529 - val_accuracy: 0.5263 - val_loss: 0.8145\n",
            "Epoch 16/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8177 - loss: 0.4624 - val_accuracy: 0.5263 - val_loss: 0.8575\n",
            "Epoch 17/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8880 - loss: 0.3105 - val_accuracy: 0.4737 - val_loss: 1.0367\n",
            "Epoch 18/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8148 - loss: 0.4718 - val_accuracy: 0.4737 - val_loss: 1.0575\n",
            "Epoch 19/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8644 - loss: 0.3806 - val_accuracy: 0.5263 - val_loss: 0.8161\n",
            "Epoch 20/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8304 - loss: 0.3681 - val_accuracy: 0.5263 - val_loss: 0.7295\n",
            "Epoch 21/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8716 - loss: 0.3366 - val_accuracy: 0.5263 - val_loss: 0.7359\n",
            "Epoch 22/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9284 - loss: 0.3023 - val_accuracy: 0.5263 - val_loss: 0.8441\n",
            "Epoch 23/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8565 - loss: 0.3362 - val_accuracy: 0.5263 - val_loss: 0.9974\n",
            "Epoch 24/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8376 - loss: 0.3252 - val_accuracy: 0.5263 - val_loss: 0.9521\n",
            "Epoch 25/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8467 - loss: 0.3043 - val_accuracy: 0.5263 - val_loss: 0.7820\n",
            "Epoch 26/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8652 - loss: 0.2860 - val_accuracy: 0.5789 - val_loss: 0.6557\n",
            "Epoch 27/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8716 - loss: 0.3640 - val_accuracy: 0.5789 - val_loss: 0.6223\n",
            "Epoch 28/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9050 - loss: 0.2948 - val_accuracy: 0.5263 - val_loss: 0.7353\n",
            "Epoch 29/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8822 - loss: 0.2798 - val_accuracy: 0.5263 - val_loss: 0.8516\n",
            "Epoch 30/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8449 - loss: 0.3286 - val_accuracy: 0.5263 - val_loss: 0.8335\n",
            "Epoch 31/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8387 - loss: 0.3260 - val_accuracy: 0.5263 - val_loss: 0.9233\n",
            "Epoch 32/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9415 - loss: 0.2520 - val_accuracy: 0.5263 - val_loss: 1.0008\n",
            "Epoch 33/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8507 - loss: 0.3259 - val_accuracy: 0.5263 - val_loss: 0.8249\n",
            "Epoch 34/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9031 - loss: 0.2691 - val_accuracy: 0.5263 - val_loss: 0.7163\n",
            "Epoch 35/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9357 - loss: 0.2364 - val_accuracy: 0.5263 - val_loss: 0.7182\n",
            "Epoch 36/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8934 - loss: 0.2634 - val_accuracy: 0.5263 - val_loss: 0.8412\n",
            "Epoch 37/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9255 - loss: 0.2518 - val_accuracy: 0.5263 - val_loss: 0.8357\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "GRU - Train Accuracy: 0.8602, Test Accuracy: 0.8000\n",
            "GRU - Train Balanced Acc: 0.5222, Test Balanced Acc: 0.5000\n",
            "GRU - Training Time: 14.54s, Memory Usage: -0.10%\n",
            "\n",
            "Training deep learning models WITHOUT macroeconomic factors:\n",
            "Training LSTM...\n",
            "Epoch 1/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 303ms/step - accuracy: 0.3079 - loss: 1.1191 - val_accuracy: 0.0526 - val_loss: 1.1967\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.7072 - loss: 1.0090 - val_accuracy: 0.0526 - val_loss: 1.2587\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7754 - loss: 0.9056 - val_accuracy: 0.0526 - val_loss: 1.3358\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8486 - loss: 0.8179 - val_accuracy: 0.0526 - val_loss: 1.4044\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8395 - loss: 0.7159 - val_accuracy: 0.1053 - val_loss: 1.4660\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8366 - loss: 0.6008 - val_accuracy: 0.1053 - val_loss: 1.5250\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8337 - loss: 0.4986 - val_accuracy: 0.1579 - val_loss: 1.5406\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8409 - loss: 0.4390 - val_accuracy: 0.1579 - val_loss: 1.5540\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8163 - loss: 0.4982 - val_accuracy: 0.1579 - val_loss: 1.5462\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 0.8127 - loss: 0.4466 - val_accuracy: 0.1579 - val_loss: 1.5262\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8467 - loss: 0.4445 - val_accuracy: 0.1579 - val_loss: 1.5169\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "LSTM - Train Accuracy: 0.6774, Test Accuracy: 0.8000\n",
            "LSTM - Train Balanced Acc: 0.2625, Test Balanced Acc: 0.5000\n",
            "LSTM - Training Time: 9.64s, Memory Usage: -0.20%\n",
            "Training GRU...\n",
            "Epoch 1/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 217ms/step - accuracy: 0.3571 - loss: 1.1436 - val_accuracy: 0.8421 - val_loss: 1.0335\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7147 - loss: 0.9908 - val_accuracy: 0.4737 - val_loss: 1.0404\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7704 - loss: 0.8776 - val_accuracy: 0.3158 - val_loss: 1.0789\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7914 - loss: 0.8034 - val_accuracy: 0.2105 - val_loss: 1.1907\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7205 - loss: 0.7008 - val_accuracy: 0.1579 - val_loss: 1.2926\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7512 - loss: 0.6119 - val_accuracy: 0.1579 - val_loss: 1.4116\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7791 - loss: 0.5257 - val_accuracy: 0.2105 - val_loss: 1.5926\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8229 - loss: 0.4673 - val_accuracy: 0.2105 - val_loss: 1.6579\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8090 - loss: 0.4325 - val_accuracy: 0.2105 - val_loss: 1.7369\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8399 - loss: 0.4262 - val_accuracy: 0.2105 - val_loss: 1.7890\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8376 - loss: 0.4464 - val_accuracy: 0.2632 - val_loss: 1.7833\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 287ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "GRU - Train Accuracy: 0.7634, Test Accuracy: 0.8000\n",
            "GRU - Train Balanced Acc: 0.6250, Test Balanced Acc: 0.5000\n",
            "GRU - Training Time: 8.77s, Memory Usage: 0.30%\n",
            "\n",
            "Simulating portfolio performance...\n",
            "\n",
            "Portfolio Performance Metrics:\n",
            "                            Total Return  Annual Return  Volatility  \\\n",
            "Buy and Hold                    0.369358       5.051592    0.588551   \n",
            "RandomForest_with_macro         0.000000       0.000000    0.000000   \n",
            "SVC_with_macro                  0.037718       0.236206    0.118895   \n",
            "XGBoost_with_macro              0.124547       0.958675    0.346077   \n",
            "LSTM_with_macro                -0.004034      -0.028681    0.008625   \n",
            "GRU_with_macro                  0.000000       0.000000    0.000000   \n",
            "RandomForest_without_macro      0.081446       0.565871    0.247588   \n",
            "SVC_without_macro              -0.132270      -0.556278    0.377218   \n",
            "XGBoost_without_macro           0.080737       0.559996    0.361611   \n",
            "LSTM_without_macro              0.000000       0.000000    0.000000   \n",
            "GRU_without_macro               0.000000       0.000000    0.000000   \n",
            "\n",
            "                            Sharpe Ratio  Max Drawdown  Win Rate  \n",
            "Buy and Hold                    8.583099     -0.142814  0.545455  \n",
            "RandomForest_with_macro         0.000000      0.000000  0.000000  \n",
            "SVC_with_macro                  1.986686     -0.021718  0.068182  \n",
            "XGBoost_with_macro              2.770120     -0.096451  0.136364  \n",
            "LSTM_with_macro                -3.325319     -0.004034  0.000000  \n",
            "GRU_with_macro                  0.000000      0.000000  0.000000  \n",
            "RandomForest_without_macro      2.285537     -0.051348  0.113636  \n",
            "SVC_without_macro              -1.474686     -0.148797  0.227273  \n",
            "XGBoost_without_macro           1.548616     -0.107923  0.136364  \n",
            "LSTM_without_macro              0.000000      0.000000  0.000000  \n",
            "GRU_without_macro               0.000000      0.000000  0.000000  \n",
            "\n",
            "Model Performance Comparison:\n",
            "          Model  Accuracy  F1 Score  Training Time (s)  Memory Usage (%)\n",
            "0  RandomForest  0.622222  0.477321           0.354648               0.0\n",
            "1           SVC  0.755556  0.717168           0.032837               0.0\n",
            "2       XGBoost  0.800000  0.790955           0.114419               0.0\n",
            "3          LSTM  0.800000  0.734426          11.775327               0.4\n",
            "4           GRU  0.800000  0.711111          14.542452              -0.1\n",
            "5  RandomForest  0.733333  0.717665           0.353472               0.0\n",
            "6           SVC  0.755556  0.758962           0.028896               0.0\n",
            "7       XGBoost  0.822222  0.816296           0.130434               0.0\n",
            "8          LSTM  0.800000  0.711111           9.644806              -0.2\n",
            "9           GRU  0.800000  0.711111           8.772635               0.3\n",
            "\n",
            "==================================================\n",
            "ANALYZING SOUTH_AFRICA\n",
            "==================================================\n",
            "Loaded south_africa data: 148 stock rows, 52 macro rows\n",
            "Available macro columns for south_africa: ['bond_yields', 'unemployment', 'gdp', 'imports', 'interest_rate', 'exports', 'exchange_rate', 'cpi']\n",
            "Successfully processed south_africa data: 148 rows, 31 columns\n",
            "Successfully processed south_africa data: 148 rows, 15 columns\n",
            "Quarterly metrics not available, using monthly metrics\n",
            "Regime distribution: {1: np.int64(90), 2: np.int64(45), 0: np.int64(13)}\n",
            "Final regime distribution: {1: np.int64(90), 2: np.int64(45), 0: np.int64(13)}\n",
            "Quarterly metrics not available, using monthly metrics\n",
            "Regime distribution: {1: np.int64(90), 2: np.int64(45), 0: np.int64(13)}\n",
            "Final regime distribution: {1: np.int64(90), 2: np.int64(45), 0: np.int64(13)}\n",
            "Removing completely empty features: ['volatility_2q']\n",
            "Removing completely empty features: ['volatility_2q']\n",
            "Features with macro: ['return_1q', 'return_2q', 'ma_ratio', 'momentum', 'gdp', 'cpi', 'exchange_rate', 'imports', 'exports', 'gdp_change', 'cpi_change', 'imports_change', 'exports_change', 'interest_rate', 'interest_rate_change']\n",
            "Features without macro: ['return_1q', 'return_2q', 'ma_ratio', 'momentum']\n",
            "Training set with macro class distribution: {1: np.int64(76), 2: np.int64(23), 0: np.int64(4)}\n",
            "Test set with macro class distribution: {2: np.int64(22), 1: np.int64(14), 0: np.int64(9)}\n",
            "Training set without macro class distribution: {1: np.int64(76), 2: np.int64(23), 0: np.int64(4)}\n",
            "Test set without macro class distribution: {2: np.int64(22), 1: np.int64(14), 0: np.int64(9)}\n",
            "\n",
            "Training traditional models WITH macroeconomic factors:\n",
            "Training RandomForest...\n",
            "RandomForest - Train Accuracy: 1.0000, Test Accuracy: 0.6667\n",
            "RandomForest - Train Balanced Acc: 1.0000, Test Balanced Acc: 0.5758\n",
            "RandomForest - Training Time: 0.57s, Memory Usage: 0.10%\n",
            "Training SVC...\n",
            "SVC - Train Accuracy: 0.8544, Test Accuracy: 0.4889\n",
            "SVC - Train Balanced Acc: 0.9342, Test Balanced Acc: 0.4545\n",
            "SVC - Training Time: 0.05s, Memory Usage: 0.00%\n",
            "Training XGBoost...\n",
            "XGBoost - Train Accuracy: 1.0000, Test Accuracy: 0.7556\n",
            "XGBoost - Train Balanced Acc: 1.0000, Test Balanced Acc: 0.6277\n",
            "XGBoost - Training Time: 0.20s, Memory Usage: 0.00%\n",
            "\n",
            "Training traditional models WITHOUT macroeconomic factors:\n",
            "Training RandomForest...\n",
            "RandomForest - Train Accuracy: 1.0000, Test Accuracy: 0.7333\n",
            "RandomForest - Train Balanced Acc: 1.0000, Test Balanced Acc: 0.6477\n",
            "RandomForest - Training Time: 0.49s, Memory Usage: 0.00%\n",
            "Training SVC...\n",
            "SVC - Train Accuracy: 0.7864, Test Accuracy: 0.6444\n",
            "SVC - Train Balanced Acc: 0.8732, Test Balanced Acc: 0.6186\n",
            "SVC - Training Time: 0.03s, Memory Usage: 0.00%\n",
            "Training XGBoost...\n",
            "XGBoost - Train Accuracy: 1.0000, Test Accuracy: 0.7556\n",
            "XGBoost - Train Balanced Acc: 1.0000, Test Balanced Acc: 0.6190\n",
            "XGBoost - Training Time: 0.10s, Memory Usage: 0.00%\n",
            "\n",
            "Training deep learning models WITH macroeconomic factors:\n",
            "Training LSTM...\n",
            "Epoch 1/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 197ms/step - accuracy: 0.3972 - loss: 1.0732 - val_accuracy: 0.1053 - val_loss: 1.1663\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7907 - loss: 0.9410 - val_accuracy: 0.1053 - val_loss: 1.2186\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7653 - loss: 0.8645 - val_accuracy: 0.1053 - val_loss: 1.2654\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7870 - loss: 0.7643 - val_accuracy: 0.1053 - val_loss: 1.3067\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8051 - loss: 0.6836 - val_accuracy: 0.1053 - val_loss: 1.3324\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8033 - loss: 0.6591 - val_accuracy: 0.1053 - val_loss: 1.3414\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8239 - loss: 0.6180 - val_accuracy: 0.1579 - val_loss: 1.3275\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7928 - loss: 0.5264 - val_accuracy: 0.2105 - val_loss: 1.3077\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8011 - loss: 0.4838 - val_accuracy: 0.2105 - val_loss: 1.3042\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8138 - loss: 0.4991 - val_accuracy: 0.2632 - val_loss: 1.2707\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8127 - loss: 0.4502 - val_accuracy: 0.2632 - val_loss: 1.2357\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "LSTM - Train Accuracy: 0.6559, Test Accuracy: 0.2857\n",
            "LSTM - Train Balanced Acc: 0.4403, Test Balanced Acc: 0.3122\n",
            "LSTM - Training Time: 7.15s, Memory Usage: 0.40%\n",
            "Training GRU...\n",
            "Epoch 1/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 638ms/step - accuracy: 0.3586 - loss: 1.0680 - val_accuracy: 0.0000e+00 - val_loss: 1.2548\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7325 - loss: 0.8453 - val_accuracy: 0.0000e+00 - val_loss: 1.5452\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7903 - loss: 0.7055 - val_accuracy: 0.0000e+00 - val_loss: 1.8018\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7723 - loss: 0.6253 - val_accuracy: 0.0000e+00 - val_loss: 2.0468\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8283 - loss: 0.5419 - val_accuracy: 0.0526 - val_loss: 2.2040\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8666 - loss: 0.4756 - val_accuracy: 0.1053 - val_loss: 2.3229\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8090 - loss: 0.4564 - val_accuracy: 0.1053 - val_loss: 2.4253\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8685 - loss: 0.4426 - val_accuracy: 0.1579 - val_loss: 2.4867\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8428 - loss: 0.4176 - val_accuracy: 0.1579 - val_loss: 2.5535\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8300 - loss: 0.4412 - val_accuracy: 0.1053 - val_loss: 2.6518\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8428 - loss: 0.3708 - val_accuracy: 0.1053 - val_loss: 2.7083\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "GRU - Train Accuracy: 0.5699, Test Accuracy: 0.2571\n",
            "GRU - Train Balanced Acc: 0.6442, Test Balanced Acc: 0.3333\n",
            "GRU - Training Time: 11.11s, Memory Usage: 0.30%\n",
            "\n",
            "Training deep learning models WITHOUT macroeconomic factors:\n",
            "Training LSTM...\n",
            "Epoch 1/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 189ms/step - accuracy: 0.1973 - loss: 1.1185 - val_accuracy: 0.8421 - val_loss: 1.0414\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6131 - loss: 1.0517 - val_accuracy: 0.6842 - val_loss: 1.0554\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6844 - loss: 0.9893 - val_accuracy: 0.2105 - val_loss: 1.0812\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6740 - loss: 0.9165 - val_accuracy: 0.1579 - val_loss: 1.1167\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6609 - loss: 0.8433 - val_accuracy: 0.1579 - val_loss: 1.1503\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7321 - loss: 0.7773 - val_accuracy: 0.1579 - val_loss: 1.2156\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7477 - loss: 0.6296 - val_accuracy: 0.1579 - val_loss: 1.2795\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6858 - loss: 0.6438 - val_accuracy: 0.2105 - val_loss: 1.2985\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6902 - loss: 0.6041 - val_accuracy: 0.1579 - val_loss: 1.3583\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7148 - loss: 0.5575 - val_accuracy: 0.1579 - val_loss: 1.3819\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6471 - loss: 0.6238 - val_accuracy: 0.1579 - val_loss: 1.3389\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "LSTM - Train Accuracy: 0.7097, Test Accuracy: 0.3143\n",
            "LSTM - Train Balanced Acc: 0.3711, Test Balanced Acc: 0.2778\n",
            "LSTM - Training Time: 8.12s, Memory Usage: 0.50%\n",
            "Training GRU...\n",
            "Epoch 1/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 548ms/step - accuracy: 0.3868 - loss: 1.0713 - val_accuracy: 0.1053 - val_loss: 1.1731\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.6116 - loss: 0.9561 - val_accuracy: 0.2105 - val_loss: 1.1572\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6149 - loss: 0.8305 - val_accuracy: 0.2105 - val_loss: 1.2122\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6261 - loss: 0.7834 - val_accuracy: 0.1579 - val_loss: 1.3438\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6160 - loss: 0.7383 - val_accuracy: 0.1053 - val_loss: 1.5365\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6877 - loss: 0.6133 - val_accuracy: 0.1053 - val_loss: 1.7555\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7304 - loss: 0.5774 - val_accuracy: 0.1053 - val_loss: 1.9203\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.6948 - loss: 0.6267 - val_accuracy: 0.1053 - val_loss: 2.1104\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7495 - loss: 0.5749 - val_accuracy: 0.1053 - val_loss: 2.2054\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7586 - loss: 0.4954 - val_accuracy: 0.1053 - val_loss: 2.0938\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7741 - loss: 0.4832 - val_accuracy: 0.2105 - val_loss: 1.9011\n",
            "Epoch 12/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7553 - loss: 0.5363 - val_accuracy: 0.2105 - val_loss: 1.9594\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 225ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "GRU - Train Accuracy: 0.5484, Test Accuracy: 0.6000\n",
            "GRU - Train Balanced Acc: 0.4869, Test Balanced Acc: 0.6415\n",
            "GRU - Training Time: 10.91s, Memory Usage: 0.40%\n",
            "\n",
            "Simulating portfolio performance...\n",
            "\n",
            "Portfolio Performance Metrics:\n",
            "                            Total Return  Annual Return  Volatility  \\\n",
            "Buy and Hold                    0.167525       1.428019    1.199063   \n",
            "RandomForest_with_macro        -0.197644      -0.716675    0.658511   \n",
            "SVC_with_macro                 -0.056999      -0.285462    0.548147   \n",
            "XGBoost_with_macro              0.001847       0.010622    0.771874   \n",
            "LSTM_with_macro                -0.172684      -0.744593    1.120770   \n",
            "GRU_with_macro                 -0.231571      -0.849911    1.240957   \n",
            "RandomForest_without_macro      0.070967       0.480931    0.778524   \n",
            "SVC_without_macro              -0.248396      -0.805125    1.073784   \n",
            "XGBoost_without_macro          -0.127476      -0.542051    0.775825   \n",
            "LSTM_without_macro              0.041502       0.340148    0.773741   \n",
            "GRU_without_macro               0.024660       0.191719    1.017807   \n",
            "\n",
            "                            Sharpe Ratio  Max Drawdown  Win Rate  \n",
            "Buy and Hold                    1.190945     -0.337035  0.568182  \n",
            "RandomForest_with_macro        -1.088327     -0.327328  0.136364  \n",
            "SVC_with_macro                 -0.520776     -0.243971  0.090909  \n",
            "XGBoost_with_macro              0.013761     -0.297009  0.250000  \n",
            "LSTM_with_macro                -0.664359     -0.351012  0.314286  \n",
            "GRU_with_macro                 -0.684884     -0.351012  0.371429  \n",
            "RandomForest_without_macro      0.617747     -0.281355  0.295455  \n",
            "SVC_without_macro              -0.749801     -0.447665  0.363636  \n",
            "XGBoost_without_macro          -0.698677     -0.337698  0.272727  \n",
            "LSTM_without_macro              0.439615     -0.161533  0.228571  \n",
            "GRU_without_macro               0.188365     -0.246038  0.342857  \n",
            "\n",
            "Model Performance Comparison:\n",
            "          Model  Accuracy  F1 Score  Training Time (s)  Memory Usage (%)\n",
            "0  RandomForest  0.666667  0.614280           0.572312               0.1\n",
            "1           SVC  0.488889  0.431547           0.047314               0.0\n",
            "2       XGBoost  0.755556  0.685285           0.195048               0.0\n",
            "3          LSTM  0.285714  0.285714           7.150402               0.4\n",
            "4           GRU  0.257143  0.105195          11.112369               0.3\n",
            "5  RandomForest  0.733333  0.716402           0.487700               0.0\n",
            "6           SVC  0.644444  0.634869           0.026939               0.0\n",
            "7       XGBoost  0.755556  0.691358           0.096171               0.0\n",
            "8          LSTM  0.314286  0.299675           8.120398               0.5\n",
            "9           GRU  0.600000  0.578349          10.913178               0.4\n",
            "\n",
            "==================================================\n",
            "CROSS-COUNTRY ANALYSIS\n",
            "==================================================\n",
            "\n",
            "Buy and Hold Performance Across Countries:\n",
            "              Sharpe Ratio  Annual Return  Max Drawdown\n",
            "brazil            1.775199       1.436538     -0.178810\n",
            "india             8.583099       5.051592     -0.142814\n",
            "south_africa      1.190945       1.428019     -0.337035\n",
            "\n",
            "RandomForest_with_macro Performance Across Countries:\n",
            "              Sharpe Ratio  Annual Return  Max Drawdown\n",
            "brazil            0.495046       0.199301     -0.146547\n",
            "india             0.000000       0.000000      0.000000\n",
            "south_africa     -1.088327      -0.716675     -0.327328\n",
            "\n",
            "SVC_with_macro Performance Across Countries:\n",
            "              Sharpe Ratio  Annual Return  Max Drawdown\n",
            "brazil           -0.335144      -0.137300     -0.211786\n",
            "india             1.986686       0.236206     -0.021718\n",
            "south_africa     -0.520776      -0.285462     -0.243971\n",
            "\n",
            "XGBoost_with_macro Performance Across Countries:\n",
            "              Sharpe Ratio  Annual Return  Max Drawdown\n",
            "brazil            1.641633       0.679348     -0.119560\n",
            "india             2.770120       0.958675     -0.096451\n",
            "south_africa      0.013761       0.010622     -0.297009\n",
            "\n",
            "LSTM_with_macro Performance Across Countries:\n",
            "              Sharpe Ratio  Annual Return  Max Drawdown\n",
            "brazil            0.000000       0.000000      0.000000\n",
            "india            -3.325319      -0.028681     -0.004034\n",
            "south_africa     -0.664359      -0.744593     -0.351012\n",
            "\n",
            "GRU_with_macro Performance Across Countries:\n",
            "              Sharpe Ratio  Annual Return  Max Drawdown\n",
            "brazil            1.181460       0.253956     -0.048893\n",
            "india             0.000000       0.000000      0.000000\n",
            "south_africa     -0.684884      -0.849911     -0.351012\n",
            "\n",
            "RandomForest_without_macro Performance Across Countries:\n",
            "              Sharpe Ratio  Annual Return  Max Drawdown\n",
            "brazil            3.195870       1.063526     -0.093285\n",
            "india             2.285537       0.565871     -0.051348\n",
            "south_africa      0.617747       0.480931     -0.281355\n",
            "\n",
            "SVC_without_macro Performance Across Countries:\n",
            "              Sharpe Ratio  Annual Return  Max Drawdown\n",
            "brazil           -0.361757      -0.224018     -0.181708\n",
            "india            -1.474686      -0.556278     -0.148797\n",
            "south_africa     -0.749801      -0.805125     -0.447665\n",
            "\n",
            "XGBoost_without_macro Performance Across Countries:\n",
            "              Sharpe Ratio  Annual Return  Max Drawdown\n",
            "brazil            3.401519       1.131831     -0.093285\n",
            "india             1.548616       0.559996     -0.107923\n",
            "south_africa     -0.698677      -0.542051     -0.337698\n",
            "\n",
            "LSTM_without_macro Performance Across Countries:\n",
            "              Sharpe Ratio  Annual Return  Max Drawdown\n",
            "brazil            7.890268       4.303455     -0.068315\n",
            "india             0.000000       0.000000      0.000000\n",
            "south_africa      0.439615       0.340148     -0.161533\n",
            "\n",
            "GRU_without_macro Performance Across Countries:\n",
            "              Sharpe Ratio  Annual Return  Max Drawdown\n",
            "brazil            1.729966       0.705680     -0.090987\n",
            "india             0.000000       0.000000      0.000000\n",
            "south_africa      0.188365       0.191719     -0.246038\n",
            "\n",
            "==================================================\n",
            "VALUE ADDED BY MACROECONOMIC FACTORS\n",
            "==================================================\n",
            "\n",
            "Value Added by Macroeconomic Factors:\n",
            "                                Country         Model  \\\n",
            "brazil_RandomForest              brazil  RandomForest   \n",
            "brazil_SVC                       brazil           SVC   \n",
            "brazil_XGBoost                   brazil       XGBoost   \n",
            "brazil_LSTM                      brazil          LSTM   \n",
            "brazil_GRU                       brazil           GRU   \n",
            "india_RandomForest                india  RandomForest   \n",
            "india_SVC                         india           SVC   \n",
            "india_XGBoost                     india       XGBoost   \n",
            "india_LSTM                        india          LSTM   \n",
            "india_GRU                         india           GRU   \n",
            "south_africa_RandomForest  south_africa  RandomForest   \n",
            "south_africa_SVC           south_africa           SVC   \n",
            "south_africa_XGBoost       south_africa       XGBoost   \n",
            "south_africa_LSTM          south_africa          LSTM   \n",
            "south_africa_GRU           south_africa           GRU   \n",
            "\n",
            "                          Sharpe Ratio Improvement Return Improvement  \\\n",
            "brazil_RandomForest                      -2.700825          -0.864225   \n",
            "brazil_SVC                                0.026613           0.086718   \n",
            "brazil_XGBoost                           -1.759887          -0.452483   \n",
            "brazil_LSTM                              -7.890268          -4.303455   \n",
            "brazil_GRU                               -0.548506          -0.451725   \n",
            "india_RandomForest                       -2.285537          -0.565871   \n",
            "india_SVC                                 3.461372           0.792484   \n",
            "india_XGBoost                             1.221504           0.398679   \n",
            "india_LSTM                               -3.325319          -0.028681   \n",
            "india_GRU                                      0.0                0.0   \n",
            "south_africa_RandomForest                -1.706074          -1.197606   \n",
            "south_africa_SVC                          0.229025           0.519663   \n",
            "south_africa_XGBoost                      0.712438           0.552673   \n",
            "south_africa_LSTM                        -1.103974          -1.084741   \n",
            "south_africa_GRU                         -0.873249           -1.04163   \n",
            "\n",
            "                          Drawdown Improvement Value Added  \n",
            "brazil_RandomForest                  -0.053262    Negative  \n",
            "brazil_SVC                           -0.030078    Positive  \n",
            "brazil_XGBoost                       -0.026275    Negative  \n",
            "brazil_LSTM                           0.068315    Negative  \n",
            "brazil_GRU                            0.042094    Negative  \n",
            "india_RandomForest                    0.051348    Negative  \n",
            "india_SVC                             0.127079    Positive  \n",
            "india_XGBoost                         0.011472    Positive  \n",
            "india_LSTM                           -0.004034    Negative  \n",
            "india_GRU                                  0.0    Negative  \n",
            "south_africa_RandomForest            -0.045972    Negative  \n",
            "south_africa_SVC                      0.203694    Positive  \n",
            "south_africa_XGBoost                  0.040689    Positive  \n",
            "south_africa_LSTM                    -0.189479    Negative  \n",
            "south_africa_GRU                     -0.104974    Negative  \n",
            "\n",
            "Regime Transition Analysis:\n",
            "\n",
            "BRAZIL:\n",
            "  Number of Transitions: 18\n",
            "  Avg Pre-Transition Return: 0.7613\n",
            "  Avg Post-Transition Return: 0.7531\n",
            "  Return Differential: -0.0083\n",
            "\n",
            "INDIA:\n",
            "  Number of Transitions: 22\n",
            "  Avg Pre-Transition Return: 0.9042\n",
            "  Avg Post-Transition Return: 0.9393\n",
            "  Return Differential: 0.0351\n",
            "\n",
            "SOUTH_AFRICA:\n",
            "  Number of Transitions: 25\n",
            "  Avg Pre-Transition Return: -0.0143\n",
            "  Avg Post-Transition Return: -0.0445\n",
            "  Return Differential: -0.0302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9L3TxjydGJ0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_HEIV_Y_DYg"
      },
      "outputs": [],
      "source": []
    }
  ]
}